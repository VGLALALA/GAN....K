{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T02:33:27.303095842Z",
     "start_time": "2024-05-18T02:33:26.543223741Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "lr = 0.0002\n",
    "batch_size = 128\n",
    "image_size = 32 * 32  # CIFAR-10 images are 32x32\n",
    "num_epochs = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T02:33:31.598640742Z",
     "start_time": "2024-05-18T02:33:27.304533789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Verifying data loading...\n",
      "Batch shape: torch.Size([128, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "os.makedirs('./data/cifar10', exist_ok=True)\n",
    "# CIFAR-10 dataset\n",
    "transform_cifar10 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training dataset\n",
    "train_dataset_cifar10 = datasets.CIFAR10(root='./data', train=True, transform=transform_cifar10, download=True)\n",
    "train_loader_cifar10 = DataLoader(dataset=train_dataset_cifar10, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# Load CIFAR-10 test dataset\n",
    "test_dataset_cifar10 = datasets.CIFAR10(root='./data', train=False, transform=transform_cifar10, download=True)\n",
    "test_loader_cifar10 = DataLoader(dataset=test_dataset_cifar10, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "# Move data to the specified device (optional)\n",
    "for images, labels in train_loader_cifar10:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    # Your training code here\n",
    "\n",
    "for images, labels in test_loader_cifar10:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "print(\"Verifying data loading...\")\n",
    "for images, labels in train_loader_cifar10:\n",
    "    print(f'Batch shape: {images.shape}')\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T02:33:31.602082802Z",
     "start_time": "2024-05-18T02:33:31.600901047Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 512 * 4 * 4))  # Projecting to a smaller space\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(32, img_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 512, 4, 4)\n",
    "        img = self.model(out)\n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, 128, kernel_size=3, stride=2, padding=1),f\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),\n",
    "        \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),\n",
    "        \n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),\n",
    "        \n",
    "            nn.Conv2d(512, 1024, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),\n",
    "        \n",
    "            nn.Conv2d(1024, 2048, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "        \n",
    "        self.adv_layer = nn.Sequential(\n",
    "            nn.Linear(2048 * 1 * 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        validity = self.adv_layer(out)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-05-18T02:33:31.603584108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Starting epoch 1/500\n",
      "Epoch [1/500], Step [100/390], D Loss: 1.3668229579925537, G Loss: 0.742272138595581\n",
      "Epoch [1/500], Step [200/390], D Loss: 1.4243297576904297, G Loss: 0.6729264259338379\n",
      "Epoch [1/500], Step [300/390], D Loss: 1.324495553970337, G Loss: 0.8380115032196045\n",
      "Starting epoch 2/500\n",
      "Epoch [2/500], Step [100/390], D Loss: 1.432347297668457, G Loss: 0.7294477224349976\n",
      "Epoch [2/500], Step [200/390], D Loss: 1.339842677116394, G Loss: 0.7587568163871765\n",
      "Epoch [2/500], Step [300/390], D Loss: 1.3487144708633423, G Loss: 0.898546040058136\n",
      "Starting epoch 3/500\n",
      "Epoch [3/500], Step [100/390], D Loss: 1.503427267074585, G Loss: 0.8152139782905579\n",
      "Epoch [3/500], Step [200/390], D Loss: 1.3597757816314697, G Loss: 0.8042879104614258\n",
      "Epoch [3/500], Step [300/390], D Loss: 1.1412049531936646, G Loss: 0.879277765750885\n",
      "Starting epoch 4/500\n",
      "Epoch [4/500], Step [100/390], D Loss: 1.1273716688156128, G Loss: 1.0551891326904297\n",
      "Epoch [4/500], Step [200/390], D Loss: 0.6962716579437256, G Loss: 1.5472112894058228\n",
      "Epoch [4/500], Step [300/390], D Loss: 1.0748212337493896, G Loss: 1.4606308937072754\n",
      "Starting epoch 5/500\n",
      "Epoch [5/500], Step [100/390], D Loss: 0.6613626480102539, G Loss: 1.3146350383758545\n",
      "Epoch [5/500], Step [200/390], D Loss: 1.322790503501892, G Loss: 1.6258189678192139\n",
      "Epoch [5/500], Step [300/390], D Loss: 0.9234591126441956, G Loss: 1.3699889183044434\n",
      "Starting epoch 6/500\n",
      "Epoch [6/500], Step [100/390], D Loss: 1.4055042266845703, G Loss: 1.296029806137085\n",
      "Epoch [6/500], Step [200/390], D Loss: 0.5950719714164734, G Loss: 2.1161210536956787\n",
      "Epoch [6/500], Step [300/390], D Loss: 1.0315476655960083, G Loss: 1.6802343130111694\n",
      "Starting epoch 7/500\n",
      "Epoch [7/500], Step [100/390], D Loss: 1.6239056587219238, G Loss: 1.0056331157684326\n",
      "Epoch [7/500], Step [200/390], D Loss: 1.123832106590271, G Loss: 1.008647084236145\n",
      "Epoch [7/500], Step [300/390], D Loss: 1.4249006509780884, G Loss: 0.9503124356269836\n",
      "Starting epoch 8/500\n",
      "Epoch [8/500], Step [100/390], D Loss: 1.3683757781982422, G Loss: 1.176530122756958\n",
      "Epoch [8/500], Step [200/390], D Loss: 1.1019182205200195, G Loss: 1.1721012592315674\n",
      "Epoch [8/500], Step [300/390], D Loss: 1.2723331451416016, G Loss: 1.4114162921905518\n",
      "Starting epoch 9/500\n",
      "Epoch [9/500], Step [100/390], D Loss: 0.9710554480552673, G Loss: 1.4127089977264404\n",
      "Epoch [9/500], Step [200/390], D Loss: 1.1940600872039795, G Loss: 1.1298861503601074\n",
      "Epoch [9/500], Step [300/390], D Loss: 0.9043231010437012, G Loss: 1.3285796642303467\n",
      "Starting epoch 10/500\n",
      "Epoch [10/500], Step [100/390], D Loss: 1.375993251800537, G Loss: 1.806317687034607\n",
      "Epoch [10/500], Step [200/390], D Loss: 1.4432649612426758, G Loss: 1.163834810256958\n",
      "Epoch [10/500], Step [300/390], D Loss: 1.1590107679367065, G Loss: 1.5937166213989258\n",
      "Starting epoch 11/500\n",
      "Epoch [11/500], Step [100/390], D Loss: 2.147749900817871, G Loss: 0.8775668740272522\n",
      "Epoch [11/500], Step [200/390], D Loss: 1.1140575408935547, G Loss: 1.042381763458252\n",
      "Epoch [11/500], Step [300/390], D Loss: 0.5365643501281738, G Loss: 1.5747768878936768\n",
      "Starting epoch 12/500\n",
      "Epoch [12/500], Step [100/390], D Loss: 1.062662124633789, G Loss: 1.713327169418335\n",
      "Epoch [12/500], Step [200/390], D Loss: 1.2560243606567383, G Loss: 1.010347843170166\n",
      "Epoch [12/500], Step [300/390], D Loss: 0.8680338859558105, G Loss: 1.0165172815322876\n",
      "Starting epoch 13/500\n",
      "Epoch [13/500], Step [100/390], D Loss: 1.118080973625183, G Loss: 1.0975528955459595\n",
      "Epoch [13/500], Step [200/390], D Loss: 1.3935532569885254, G Loss: 1.1561009883880615\n",
      "Epoch [13/500], Step [300/390], D Loss: 1.3155605792999268, G Loss: 1.164081335067749\n",
      "Starting epoch 14/500\n",
      "Epoch [14/500], Step [100/390], D Loss: 1.360259771347046, G Loss: 0.9354658722877502\n",
      "Epoch [14/500], Step [200/390], D Loss: 0.9698062539100647, G Loss: 1.385488748550415\n",
      "Epoch [14/500], Step [300/390], D Loss: 0.6259823441505432, G Loss: 1.4155733585357666\n",
      "Starting epoch 15/500\n",
      "Epoch [15/500], Step [100/390], D Loss: 1.0183273553848267, G Loss: 1.5285483598709106\n",
      "Epoch [15/500], Step [200/390], D Loss: 0.6812916994094849, G Loss: 1.885826826095581\n",
      "Epoch [15/500], Step [300/390], D Loss: 1.2775440216064453, G Loss: 1.3205206394195557\n",
      "Starting epoch 16/500\n",
      "Epoch [16/500], Step [100/390], D Loss: 0.7843275666236877, G Loss: 1.5006437301635742\n",
      "Epoch [16/500], Step [200/390], D Loss: 1.0213539600372314, G Loss: 1.1658731698989868\n",
      "Epoch [16/500], Step [300/390], D Loss: 0.609392523765564, G Loss: 2.2025833129882812\n",
      "Starting epoch 17/500\n",
      "Epoch [17/500], Step [100/390], D Loss: 0.985745370388031, G Loss: 1.4680709838867188\n",
      "Epoch [17/500], Step [200/390], D Loss: 1.1967978477478027, G Loss: 1.0071297883987427\n",
      "Epoch [17/500], Step [300/390], D Loss: 1.125922441482544, G Loss: 1.095220923423767\n",
      "Starting epoch 18/500\n",
      "Epoch [18/500], Step [100/390], D Loss: 0.8449209928512573, G Loss: 1.4685733318328857\n",
      "Epoch [18/500], Step [200/390], D Loss: 0.9160341024398804, G Loss: 1.4818081855773926\n",
      "Epoch [18/500], Step [300/390], D Loss: 1.075352430343628, G Loss: 1.1278363466262817\n",
      "Starting epoch 19/500\n",
      "Epoch [19/500], Step [100/390], D Loss: 1.9902446269989014, G Loss: 1.1126866340637207\n",
      "Epoch [19/500], Step [200/390], D Loss: 1.2487497329711914, G Loss: 1.0052711963653564\n",
      "Epoch [19/500], Step [300/390], D Loss: 0.9650801420211792, G Loss: 1.2244516611099243\n",
      "Starting epoch 20/500\n",
      "Epoch [20/500], Step [100/390], D Loss: 1.6426515579223633, G Loss: 1.074967622756958\n",
      "Epoch [20/500], Step [200/390], D Loss: 1.0705888271331787, G Loss: 1.0761617422103882\n",
      "Epoch [20/500], Step [300/390], D Loss: 1.2102961540222168, G Loss: 1.7047665119171143\n",
      "Starting epoch 21/500\n",
      "Epoch [21/500], Step [100/390], D Loss: 1.4887018203735352, G Loss: 1.1030261516571045\n",
      "Epoch [21/500], Step [200/390], D Loss: 0.9510869979858398, G Loss: 1.6461493968963623\n",
      "Epoch [21/500], Step [300/390], D Loss: 0.9373670816421509, G Loss: 1.4840387105941772\n",
      "Starting epoch 22/500\n",
      "Epoch [22/500], Step [100/390], D Loss: 0.5066110491752625, G Loss: 1.8580749034881592\n",
      "Epoch [22/500], Step [200/390], D Loss: 0.6167243719100952, G Loss: 2.193250894546509\n",
      "Epoch [22/500], Step [300/390], D Loss: 1.0672277212142944, G Loss: 1.2458229064941406\n",
      "Starting epoch 23/500\n",
      "Epoch [23/500], Step [100/390], D Loss: 1.2282733917236328, G Loss: 1.7585909366607666\n",
      "Epoch [23/500], Step [200/390], D Loss: 0.9705630540847778, G Loss: 1.2284566164016724\n",
      "Epoch [23/500], Step [300/390], D Loss: 1.2086052894592285, G Loss: 1.1022722721099854\n",
      "Starting epoch 24/500\n",
      "Epoch [24/500], Step [100/390], D Loss: 0.9212521314620972, G Loss: 1.234798550605774\n",
      "Epoch [24/500], Step [200/390], D Loss: 1.6024119853973389, G Loss: 1.0969432592391968\n",
      "Epoch [24/500], Step [300/390], D Loss: 1.6860935688018799, G Loss: 1.2827990055084229\n",
      "Starting epoch 25/500\n",
      "Epoch [25/500], Step [100/390], D Loss: 0.9708747863769531, G Loss: 1.457783818244934\n",
      "Epoch [25/500], Step [200/390], D Loss: 0.9778726100921631, G Loss: 1.7226240634918213\n",
      "Epoch [25/500], Step [300/390], D Loss: 1.0322386026382446, G Loss: 1.7915441989898682\n",
      "Starting epoch 26/500\n",
      "Epoch [26/500], Step [100/390], D Loss: 1.217292070388794, G Loss: 1.877392292022705\n",
      "Epoch [26/500], Step [200/390], D Loss: 0.6942160725593567, G Loss: 1.747726321220398\n",
      "Epoch [26/500], Step [300/390], D Loss: 0.4383522570133209, G Loss: 2.190674066543579\n",
      "Starting epoch 27/500\n",
      "Epoch [27/500], Step [100/390], D Loss: 1.1120777130126953, G Loss: 1.4342353343963623\n",
      "Epoch [27/500], Step [200/390], D Loss: 0.9824331402778625, G Loss: 1.8978301286697388\n",
      "Epoch [27/500], Step [300/390], D Loss: 1.631838321685791, G Loss: 1.5159296989440918\n",
      "Starting epoch 28/500\n",
      "Epoch [28/500], Step [100/390], D Loss: 1.475834608078003, G Loss: 1.16853666305542\n",
      "Epoch [28/500], Step [200/390], D Loss: 1.5836008787155151, G Loss: 1.2558274269104004\n",
      "Epoch [28/500], Step [300/390], D Loss: 1.1361570358276367, G Loss: 1.6939589977264404\n",
      "Starting epoch 29/500\n",
      "Epoch [29/500], Step [100/390], D Loss: 1.1452662944793701, G Loss: 1.1594626903533936\n",
      "Epoch [29/500], Step [200/390], D Loss: 0.8204363584518433, G Loss: 0.9997458457946777\n",
      "Epoch [29/500], Step [300/390], D Loss: 0.931255042552948, G Loss: 1.7715715169906616\n",
      "Starting epoch 30/500\n",
      "Epoch [30/500], Step [100/390], D Loss: 0.6811578869819641, G Loss: 1.7516355514526367\n",
      "Epoch [30/500], Step [200/390], D Loss: 0.8305497169494629, G Loss: 1.8213167190551758\n",
      "Epoch [30/500], Step [300/390], D Loss: 1.217246651649475, G Loss: 1.9389009475708008\n",
      "Starting epoch 31/500\n",
      "Epoch [31/500], Step [100/390], D Loss: 1.0435988903045654, G Loss: 1.6953251361846924\n",
      "Epoch [31/500], Step [200/390], D Loss: 1.0992929935455322, G Loss: 2.228017807006836\n",
      "Epoch [31/500], Step [300/390], D Loss: 0.9809976816177368, G Loss: 1.1384589672088623\n",
      "Starting epoch 32/500\n",
      "Epoch [32/500], Step [100/390], D Loss: 0.4238503575325012, G Loss: 2.382361888885498\n",
      "Epoch [32/500], Step [200/390], D Loss: 0.6543222665786743, G Loss: 1.90437912940979\n",
      "Epoch [32/500], Step [300/390], D Loss: 1.0967357158660889, G Loss: 1.7678364515304565\n",
      "Starting epoch 33/500\n",
      "Epoch [33/500], Step [100/390], D Loss: 1.001645803451538, G Loss: 1.399498701095581\n",
      "Epoch [33/500], Step [200/390], D Loss: 1.0579777956008911, G Loss: 1.508758783340454\n",
      "Epoch [33/500], Step [300/390], D Loss: 0.5173116326332092, G Loss: 1.9180999994277954\n",
      "Starting epoch 34/500\n",
      "Epoch [34/500], Step [100/390], D Loss: 1.1056205034255981, G Loss: 1.4207401275634766\n",
      "Epoch [34/500], Step [200/390], D Loss: 0.7556572556495667, G Loss: 1.7204924821853638\n",
      "Epoch [34/500], Step [300/390], D Loss: 0.8594592809677124, G Loss: 1.7558443546295166\n",
      "Starting epoch 35/500\n",
      "Epoch [35/500], Step [100/390], D Loss: 2.284555196762085, G Loss: 0.8074901103973389\n",
      "Epoch [35/500], Step [200/390], D Loss: 0.5584077835083008, G Loss: 1.8942227363586426\n",
      "Epoch [35/500], Step [300/390], D Loss: 0.9122278690338135, G Loss: 1.6502487659454346\n",
      "Starting epoch 36/500\n",
      "Epoch [36/500], Step [100/390], D Loss: 0.941726803779602, G Loss: 1.3475275039672852\n",
      "Epoch [36/500], Step [200/390], D Loss: 0.4919205605983734, G Loss: 1.788412094116211\n",
      "Epoch [36/500], Step [300/390], D Loss: 1.2088673114776611, G Loss: 1.3288333415985107\n",
      "Starting epoch 37/500\n",
      "Epoch [37/500], Step [100/390], D Loss: 0.8348268866539001, G Loss: 1.2427887916564941\n",
      "Epoch [37/500], Step [200/390], D Loss: 1.258850336074829, G Loss: 0.994914710521698\n",
      "Epoch [37/500], Step [300/390], D Loss: 1.6768765449523926, G Loss: 1.1614410877227783\n",
      "Starting epoch 38/500\n",
      "Epoch [38/500], Step [100/390], D Loss: 1.0267914533615112, G Loss: 1.204129934310913\n",
      "Epoch [38/500], Step [200/390], D Loss: 0.5889762043952942, G Loss: 1.5754319429397583\n",
      "Epoch [38/500], Step [300/390], D Loss: 1.6577024459838867, G Loss: 0.9527584314346313\n",
      "Starting epoch 39/500\n",
      "Epoch [39/500], Step [100/390], D Loss: 0.9029312133789062, G Loss: 1.3490381240844727\n",
      "Epoch [39/500], Step [200/390], D Loss: 0.9514138102531433, G Loss: 1.5882444381713867\n",
      "Epoch [39/500], Step [300/390], D Loss: 1.1623375415802002, G Loss: 1.2167781591415405\n",
      "Starting epoch 40/500\n",
      "Epoch [40/500], Step [100/390], D Loss: 1.13314688205719, G Loss: 1.1121859550476074\n",
      "Epoch [40/500], Step [200/390], D Loss: 0.6837883591651917, G Loss: 1.6537472009658813\n",
      "Epoch [40/500], Step [300/390], D Loss: 1.4956775903701782, G Loss: 1.177736520767212\n",
      "Starting epoch 41/500\n",
      "Epoch [41/500], Step [100/390], D Loss: 0.5129519104957581, G Loss: 1.6522246599197388\n",
      "Epoch [41/500], Step [200/390], D Loss: 0.8490840196609497, G Loss: 1.3456294536590576\n",
      "Epoch [41/500], Step [300/390], D Loss: 0.6991521120071411, G Loss: 1.5094714164733887\n",
      "Starting epoch 42/500\n",
      "Epoch [42/500], Step [100/390], D Loss: 1.3443639278411865, G Loss: 1.588470458984375\n",
      "Epoch [42/500], Step [200/390], D Loss: 0.6304272413253784, G Loss: 1.763676643371582\n",
      "Epoch [42/500], Step [300/390], D Loss: 1.1836283206939697, G Loss: 1.6172800064086914\n",
      "Starting epoch 43/500\n",
      "Epoch [43/500], Step [100/390], D Loss: 1.2338823080062866, G Loss: 1.2094080448150635\n",
      "Epoch [43/500], Step [200/390], D Loss: 0.7405259013175964, G Loss: 1.3729114532470703\n",
      "Epoch [43/500], Step [300/390], D Loss: 1.3772999048233032, G Loss: 1.7241675853729248\n",
      "Starting epoch 44/500\n",
      "Epoch [44/500], Step [100/390], D Loss: 0.6301997900009155, G Loss: 1.883774757385254\n",
      "Epoch [44/500], Step [200/390], D Loss: 0.7262641191482544, G Loss: 1.5956006050109863\n",
      "Epoch [44/500], Step [300/390], D Loss: 0.6757017374038696, G Loss: 1.7696887254714966\n",
      "Starting epoch 45/500\n",
      "Epoch [45/500], Step [100/390], D Loss: 0.640174388885498, G Loss: 2.337615728378296\n",
      "Epoch [45/500], Step [200/390], D Loss: 1.2296444177627563, G Loss: 1.6997601985931396\n",
      "Epoch [45/500], Step [300/390], D Loss: 1.0126317739486694, G Loss: 1.6312379837036133\n",
      "Starting epoch 46/500\n",
      "Epoch [46/500], Step [100/390], D Loss: 0.7748172879219055, G Loss: 1.6546857357025146\n",
      "Epoch [46/500], Step [200/390], D Loss: 1.1553876399993896, G Loss: 1.5077472925186157\n",
      "Epoch [46/500], Step [300/390], D Loss: 0.8973520398139954, G Loss: 1.4173136949539185\n",
      "Starting epoch 47/500\n",
      "Epoch [47/500], Step [100/390], D Loss: 1.1989355087280273, G Loss: 1.3559269905090332\n",
      "Epoch [47/500], Step [200/390], D Loss: 0.6692205667495728, G Loss: 1.8080071210861206\n",
      "Epoch [47/500], Step [300/390], D Loss: 0.9187734127044678, G Loss: 1.5366231203079224\n",
      "Starting epoch 48/500\n",
      "Epoch [48/500], Step [100/390], D Loss: 1.3391706943511963, G Loss: 1.2056446075439453\n",
      "Epoch [48/500], Step [200/390], D Loss: 0.7278425097465515, G Loss: 1.767674446105957\n",
      "Epoch [48/500], Step [300/390], D Loss: 1.3740057945251465, G Loss: 1.37091863155365\n",
      "Starting epoch 49/500\n",
      "Epoch [49/500], Step [100/390], D Loss: 0.7649362087249756, G Loss: 1.8694052696228027\n",
      "Epoch [49/500], Step [200/390], D Loss: 0.9153750538825989, G Loss: 2.352342128753662\n",
      "Epoch [49/500], Step [300/390], D Loss: 1.0266515016555786, G Loss: 1.8483154773712158\n",
      "Starting epoch 50/500\n",
      "Epoch [50/500], Step [100/390], D Loss: 2.2117738723754883, G Loss: 0.9533959031105042\n",
      "Epoch [50/500], Step [200/390], D Loss: 1.145555019378662, G Loss: 1.0980814695358276\n",
      "Epoch [50/500], Step [300/390], D Loss: 0.8640040159225464, G Loss: 1.30020272731781\n",
      "Starting epoch 51/500\n",
      "Epoch [51/500], Step [100/390], D Loss: 0.5792486667633057, G Loss: 1.4445730447769165\n",
      "Epoch [51/500], Step [200/390], D Loss: 1.0169111490249634, G Loss: 0.9676333665847778\n",
      "Epoch [51/500], Step [300/390], D Loss: 1.0762001276016235, G Loss: 1.2923210859298706\n",
      "Starting epoch 52/500\n",
      "Epoch [52/500], Step [100/390], D Loss: 0.888740599155426, G Loss: 1.9104013442993164\n",
      "Epoch [52/500], Step [200/390], D Loss: 1.2416276931762695, G Loss: 1.4219660758972168\n",
      "Epoch [52/500], Step [300/390], D Loss: 0.9959731698036194, G Loss: 0.8524017333984375\n",
      "Starting epoch 53/500\n",
      "Epoch [53/500], Step [100/390], D Loss: 1.1293737888336182, G Loss: 1.4944530725479126\n",
      "Epoch [53/500], Step [200/390], D Loss: 0.7728588581085205, G Loss: 1.9431142807006836\n",
      "Epoch [53/500], Step [300/390], D Loss: 1.2401301860809326, G Loss: 1.3464510440826416\n",
      "Starting epoch 54/500\n",
      "Epoch [54/500], Step [100/390], D Loss: 1.0152239799499512, G Loss: 1.246293544769287\n",
      "Epoch [54/500], Step [200/390], D Loss: 0.8699923753738403, G Loss: 1.8187881708145142\n",
      "Epoch [54/500], Step [300/390], D Loss: 1.1557172536849976, G Loss: 1.4526395797729492\n",
      "Starting epoch 55/500\n",
      "Epoch [55/500], Step [100/390], D Loss: 0.7524986267089844, G Loss: 1.4330191612243652\n",
      "Epoch [55/500], Step [200/390], D Loss: 1.1001462936401367, G Loss: 1.5964319705963135\n",
      "Epoch [55/500], Step [300/390], D Loss: 1.027082920074463, G Loss: 1.267219066619873\n",
      "Starting epoch 56/500\n",
      "Epoch [56/500], Step [100/390], D Loss: 1.0213998556137085, G Loss: 1.3057128190994263\n",
      "Epoch [56/500], Step [200/390], D Loss: 1.575919508934021, G Loss: 1.5970065593719482\n",
      "Epoch [56/500], Step [300/390], D Loss: 1.1508928537368774, G Loss: 1.4568467140197754\n",
      "Starting epoch 57/500\n",
      "Epoch [57/500], Step [100/390], D Loss: 0.8791695833206177, G Loss: 1.5533791780471802\n",
      "Epoch [57/500], Step [200/390], D Loss: 0.47655993700027466, G Loss: 1.7437851428985596\n",
      "Epoch [57/500], Step [300/390], D Loss: 1.08785080909729, G Loss: 1.7367534637451172\n",
      "Starting epoch 58/500\n",
      "Epoch [58/500], Step [100/390], D Loss: 0.6965574026107788, G Loss: 2.3132567405700684\n",
      "Epoch [58/500], Step [200/390], D Loss: 0.8100153207778931, G Loss: 1.948766827583313\n",
      "Epoch [58/500], Step [300/390], D Loss: 1.387617826461792, G Loss: 1.7192699909210205\n",
      "Starting epoch 59/500\n",
      "Epoch [59/500], Step [100/390], D Loss: 1.0001133680343628, G Loss: 1.2578773498535156\n",
      "Epoch [59/500], Step [200/390], D Loss: 0.973637580871582, G Loss: 1.2221548557281494\n",
      "Epoch [59/500], Step [300/390], D Loss: 1.4610460996627808, G Loss: 1.0417178869247437\n",
      "Starting epoch 60/500\n",
      "Epoch [60/500], Step [100/390], D Loss: 1.2927024364471436, G Loss: 1.094252586364746\n",
      "Epoch [60/500], Step [200/390], D Loss: 1.1743438243865967, G Loss: 1.0746220350265503\n",
      "Epoch [60/500], Step [300/390], D Loss: 1.1817845106124878, G Loss: 2.3516154289245605\n",
      "Starting epoch 61/500\n",
      "Epoch [61/500], Step [100/390], D Loss: 1.1193289756774902, G Loss: 1.536710500717163\n",
      "Epoch [61/500], Step [200/390], D Loss: 1.1242951154708862, G Loss: 1.0410363674163818\n",
      "Epoch [61/500], Step [300/390], D Loss: 1.053581953048706, G Loss: 1.3380844593048096\n",
      "Starting epoch 62/500\n",
      "Epoch [62/500], Step [100/390], D Loss: 1.384709119796753, G Loss: 1.3726704120635986\n",
      "Epoch [62/500], Step [200/390], D Loss: 1.2178996801376343, G Loss: 1.4095532894134521\n",
      "Epoch [62/500], Step [300/390], D Loss: 0.8603745698928833, G Loss: 1.6144440174102783\n",
      "Starting epoch 63/500\n",
      "Epoch [63/500], Step [100/390], D Loss: 2.0616531372070312, G Loss: 1.398918867111206\n",
      "Epoch [63/500], Step [200/390], D Loss: 1.4418803453445435, G Loss: 1.4353065490722656\n",
      "Epoch [63/500], Step [300/390], D Loss: 1.4206639528274536, G Loss: 1.5090348720550537\n",
      "Starting epoch 64/500\n",
      "Epoch [64/500], Step [100/390], D Loss: 1.0841383934020996, G Loss: 1.215615153312683\n",
      "Epoch [64/500], Step [200/390], D Loss: 1.098076343536377, G Loss: 0.9985711574554443\n",
      "Epoch [64/500], Step [300/390], D Loss: 1.6842560768127441, G Loss: 1.0185339450836182\n",
      "Starting epoch 65/500\n",
      "Epoch [65/500], Step [100/390], D Loss: 1.2904677391052246, G Loss: 0.7698420882225037\n",
      "Epoch [65/500], Step [200/390], D Loss: 0.9210739135742188, G Loss: 1.261866569519043\n",
      "Epoch [65/500], Step [300/390], D Loss: 1.4155685901641846, G Loss: 1.2606158256530762\n",
      "Starting epoch 66/500\n",
      "Epoch [66/500], Step [100/390], D Loss: 0.7782813906669617, G Loss: 1.4453024864196777\n",
      "Epoch [66/500], Step [200/390], D Loss: 0.6553605794906616, G Loss: 1.7056090831756592\n",
      "Epoch [66/500], Step [300/390], D Loss: 0.9826450943946838, G Loss: 1.5671191215515137\n",
      "Starting epoch 67/500\n",
      "Epoch [67/500], Step [100/390], D Loss: 0.669407844543457, G Loss: 1.4307332038879395\n",
      "Epoch [67/500], Step [200/390], D Loss: 1.800400972366333, G Loss: 1.0077908039093018\n",
      "Epoch [67/500], Step [300/390], D Loss: 1.2737958431243896, G Loss: 1.1130276918411255\n",
      "Starting epoch 68/500\n",
      "Epoch [68/500], Step [100/390], D Loss: 1.5348267555236816, G Loss: 1.2275466918945312\n",
      "Epoch [68/500], Step [200/390], D Loss: 1.2814815044403076, G Loss: 0.9133763909339905\n",
      "Epoch [68/500], Step [300/390], D Loss: 0.9009908437728882, G Loss: 0.970886766910553\n",
      "Starting epoch 69/500\n",
      "Epoch [69/500], Step [100/390], D Loss: 1.158803939819336, G Loss: 0.8610271215438843\n",
      "Epoch [69/500], Step [200/390], D Loss: 0.952649712562561, G Loss: 1.6766046285629272\n",
      "Epoch [69/500], Step [300/390], D Loss: 2.3231983184814453, G Loss: 1.5741746425628662\n",
      "Starting epoch 70/500\n",
      "Epoch [70/500], Step [100/390], D Loss: 1.3687520027160645, G Loss: 0.7127370238304138\n",
      "Epoch [70/500], Step [200/390], D Loss: 1.2754873037338257, G Loss: 1.3911769390106201\n",
      "Epoch [70/500], Step [300/390], D Loss: 0.7198992371559143, G Loss: 1.793260097503662\n",
      "Starting epoch 71/500\n",
      "Epoch [71/500], Step [100/390], D Loss: 0.5262326002120972, G Loss: 1.3868749141693115\n",
      "Epoch [71/500], Step [200/390], D Loss: 0.7305021286010742, G Loss: 1.3184192180633545\n",
      "Epoch [71/500], Step [300/390], D Loss: 1.1575756072998047, G Loss: 1.1053959131240845\n",
      "Starting epoch 72/500\n",
      "Epoch [72/500], Step [100/390], D Loss: 0.6653537750244141, G Loss: 1.7114086151123047\n",
      "Epoch [72/500], Step [200/390], D Loss: 0.7720192074775696, G Loss: 1.922088861465454\n",
      "Epoch [72/500], Step [300/390], D Loss: 1.181837558746338, G Loss: 2.348029136657715\n",
      "Starting epoch 73/500\n",
      "Epoch [73/500], Step [100/390], D Loss: 1.263513445854187, G Loss: 0.967394232749939\n",
      "Epoch [73/500], Step [200/390], D Loss: 0.7837182283401489, G Loss: 2.0387115478515625\n",
      "Epoch [73/500], Step [300/390], D Loss: 0.8384707570075989, G Loss: 2.1047375202178955\n",
      "Starting epoch 74/500\n",
      "Epoch [74/500], Step [100/390], D Loss: 1.1560242176055908, G Loss: 1.228898286819458\n",
      "Epoch [74/500], Step [200/390], D Loss: 1.3928550481796265, G Loss: 1.4280046224594116\n",
      "Epoch [74/500], Step [300/390], D Loss: 1.5825624465942383, G Loss: 1.8225462436676025\n",
      "Starting epoch 75/500\n",
      "Epoch [75/500], Step [100/390], D Loss: 0.8504582047462463, G Loss: 1.386176347732544\n",
      "Epoch [75/500], Step [200/390], D Loss: 0.5104377269744873, G Loss: 1.9896501302719116\n",
      "Epoch [75/500], Step [300/390], D Loss: 1.3231048583984375, G Loss: 1.0266653299331665\n",
      "Starting epoch 76/500\n",
      "Epoch [76/500], Step [100/390], D Loss: 1.7600047588348389, G Loss: 1.2397547960281372\n",
      "Epoch [76/500], Step [200/390], D Loss: 0.8880906105041504, G Loss: 1.0644831657409668\n",
      "Epoch [76/500], Step [300/390], D Loss: 1.290860891342163, G Loss: 0.4921194612979889\n",
      "Starting epoch 77/500\n",
      "Epoch [77/500], Step [100/390], D Loss: 0.9125492572784424, G Loss: 1.6525144577026367\n",
      "Epoch [77/500], Step [200/390], D Loss: 1.2846282720565796, G Loss: 1.417894721031189\n",
      "Epoch [77/500], Step [300/390], D Loss: 1.0367906093597412, G Loss: 2.190584421157837\n",
      "Starting epoch 78/500\n",
      "Epoch [78/500], Step [100/390], D Loss: 0.7112900018692017, G Loss: 1.2057831287384033\n",
      "Epoch [78/500], Step [200/390], D Loss: 1.2479201555252075, G Loss: 1.7804832458496094\n",
      "Epoch [78/500], Step [300/390], D Loss: 0.9762830138206482, G Loss: 1.3353054523468018\n",
      "Starting epoch 79/500\n",
      "Epoch [79/500], Step [100/390], D Loss: 0.7092474102973938, G Loss: 1.3274494409561157\n",
      "Epoch [79/500], Step [200/390], D Loss: 0.6077790260314941, G Loss: 2.4105563163757324\n",
      "Epoch [79/500], Step [300/390], D Loss: 1.3644944429397583, G Loss: 1.237450122833252\n",
      "Starting epoch 80/500\n",
      "Epoch [80/500], Step [100/390], D Loss: 0.7447314262390137, G Loss: 1.3887429237365723\n",
      "Epoch [80/500], Step [200/390], D Loss: 0.9450289607048035, G Loss: 1.141772985458374\n",
      "Epoch [80/500], Step [300/390], D Loss: 1.1280908584594727, G Loss: 2.029810905456543\n",
      "Starting epoch 81/500\n",
      "Epoch [81/500], Step [100/390], D Loss: 0.6869948506355286, G Loss: 1.5725154876708984\n",
      "Epoch [81/500], Step [200/390], D Loss: 1.6548867225646973, G Loss: 1.4827871322631836\n",
      "Epoch [81/500], Step [300/390], D Loss: 0.5932103395462036, G Loss: 1.8939709663391113\n",
      "Starting epoch 82/500\n",
      "Epoch [82/500], Step [100/390], D Loss: 1.037381649017334, G Loss: 1.9164414405822754\n",
      "Epoch [82/500], Step [200/390], D Loss: 1.1794326305389404, G Loss: 1.2215019464492798\n",
      "Epoch [82/500], Step [300/390], D Loss: 1.2655701637268066, G Loss: 0.954872190952301\n",
      "Starting epoch 83/500\n",
      "Epoch [83/500], Step [100/390], D Loss: 0.943853497505188, G Loss: 1.6090030670166016\n",
      "Epoch [83/500], Step [200/390], D Loss: 1.1311616897583008, G Loss: 1.095166563987732\n",
      "Epoch [83/500], Step [300/390], D Loss: 1.0682486295700073, G Loss: 1.3700428009033203\n",
      "Starting epoch 84/500\n",
      "Epoch [84/500], Step [100/390], D Loss: 1.3269399404525757, G Loss: 1.6398062705993652\n",
      "Epoch [84/500], Step [200/390], D Loss: 1.1938838958740234, G Loss: 1.2741167545318604\n",
      "Epoch [84/500], Step [300/390], D Loss: 0.664720356464386, G Loss: 1.5062124729156494\n",
      "Starting epoch 85/500\n",
      "Epoch [85/500], Step [100/390], D Loss: 1.3910727500915527, G Loss: 1.8338170051574707\n",
      "Epoch [85/500], Step [200/390], D Loss: 1.713972568511963, G Loss: 1.2825671434402466\n",
      "Epoch [85/500], Step [300/390], D Loss: 0.6504381895065308, G Loss: 1.8096446990966797\n",
      "Starting epoch 86/500\n",
      "Epoch [86/500], Step [100/390], D Loss: 0.9979737997055054, G Loss: 2.0738348960876465\n",
      "Epoch [86/500], Step [200/390], D Loss: 0.8208271265029907, G Loss: 1.4964548349380493\n",
      "Epoch [86/500], Step [300/390], D Loss: 0.5448223352432251, G Loss: 1.5043346881866455\n",
      "Starting epoch 87/500\n",
      "Epoch [87/500], Step [100/390], D Loss: 1.1928231716156006, G Loss: 1.171888828277588\n",
      "Epoch [87/500], Step [200/390], D Loss: 0.987114429473877, G Loss: 1.1737658977508545\n",
      "Epoch [87/500], Step [300/390], D Loss: 0.5507728457450867, G Loss: 1.7747682332992554\n",
      "Starting epoch 88/500\n",
      "Epoch [88/500], Step [100/390], D Loss: 0.6275571584701538, G Loss: 1.6150922775268555\n",
      "Epoch [88/500], Step [200/390], D Loss: 1.4590613842010498, G Loss: 1.7510175704956055\n",
      "Epoch [88/500], Step [300/390], D Loss: 0.4471108913421631, G Loss: 2.2388384342193604\n",
      "Starting epoch 89/500\n",
      "Epoch [89/500], Step [100/390], D Loss: 1.6341439485549927, G Loss: 1.3660879135131836\n",
      "Epoch [89/500], Step [200/390], D Loss: 0.45615559816360474, G Loss: 1.9100419282913208\n",
      "Epoch [89/500], Step [300/390], D Loss: 0.6532926559448242, G Loss: 0.9574587345123291\n",
      "Starting epoch 90/500\n",
      "Epoch [90/500], Step [100/390], D Loss: 0.5004571676254272, G Loss: 2.328263759613037\n",
      "Epoch [90/500], Step [200/390], D Loss: 1.0538921356201172, G Loss: 1.769897222518921\n",
      "Epoch [90/500], Step [300/390], D Loss: 1.0365643501281738, G Loss: 1.1877497434616089\n",
      "Starting epoch 91/500\n",
      "Epoch [91/500], Step [100/390], D Loss: 0.8218250870704651, G Loss: 1.8197828531265259\n",
      "Epoch [91/500], Step [200/390], D Loss: 0.7791895270347595, G Loss: 1.9452975988388062\n",
      "Epoch [91/500], Step [300/390], D Loss: 0.4979321360588074, G Loss: 1.2815557718276978\n",
      "Starting epoch 92/500\n",
      "Epoch [92/500], Step [100/390], D Loss: 0.852948784828186, G Loss: 2.3927249908447266\n",
      "Epoch [92/500], Step [200/390], D Loss: 1.629453182220459, G Loss: 1.1698331832885742\n",
      "Epoch [92/500], Step [300/390], D Loss: 0.7784313559532166, G Loss: 1.4900877475738525\n",
      "Starting epoch 93/500\n",
      "Epoch [93/500], Step [100/390], D Loss: 1.950502872467041, G Loss: 1.7403957843780518\n",
      "Epoch [93/500], Step [200/390], D Loss: 0.8896703124046326, G Loss: 1.442921757698059\n",
      "Epoch [93/500], Step [300/390], D Loss: 0.3958459496498108, G Loss: 2.618037462234497\n",
      "Starting epoch 94/500\n",
      "Epoch [94/500], Step [100/390], D Loss: 0.5613206624984741, G Loss: 2.288879632949829\n",
      "Epoch [94/500], Step [200/390], D Loss: 0.783207893371582, G Loss: 2.2078869342803955\n",
      "Epoch [94/500], Step [300/390], D Loss: 0.6778514385223389, G Loss: 1.517946720123291\n",
      "Starting epoch 95/500\n",
      "Epoch [95/500], Step [100/390], D Loss: 1.503963589668274, G Loss: 1.783846378326416\n",
      "Epoch [95/500], Step [200/390], D Loss: 0.8112879991531372, G Loss: 2.394505500793457\n",
      "Epoch [95/500], Step [300/390], D Loss: 0.3563782572746277, G Loss: 2.4395580291748047\n",
      "Starting epoch 96/500\n",
      "Epoch [96/500], Step [100/390], D Loss: 0.2766653299331665, G Loss: 3.1073265075683594\n",
      "Epoch [96/500], Step [200/390], D Loss: 0.26265406608581543, G Loss: 3.9391424655914307\n",
      "Epoch [96/500], Step [300/390], D Loss: 0.5628837943077087, G Loss: 1.5845415592193604\n",
      "Starting epoch 97/500\n",
      "Epoch [97/500], Step [100/390], D Loss: 0.9944840669631958, G Loss: 3.10514760017395\n",
      "Epoch [97/500], Step [200/390], D Loss: 0.40002205967903137, G Loss: 2.59135365486145\n",
      "Epoch [97/500], Step [300/390], D Loss: 0.728562593460083, G Loss: 1.282942771911621\n",
      "Starting epoch 98/500\n",
      "Epoch [98/500], Step [100/390], D Loss: 0.6242355704307556, G Loss: 2.1549367904663086\n",
      "Epoch [98/500], Step [200/390], D Loss: 0.48755109310150146, G Loss: 2.9075002670288086\n",
      "Epoch [98/500], Step [300/390], D Loss: 0.7744307518005371, G Loss: 1.4580985307693481\n",
      "Starting epoch 99/500\n",
      "Epoch [99/500], Step [100/390], D Loss: 0.5103775858879089, G Loss: 2.7609615325927734\n",
      "Epoch [99/500], Step [200/390], D Loss: 0.39059263467788696, G Loss: 2.3084330558776855\n",
      "Epoch [99/500], Step [300/390], D Loss: 0.7899137139320374, G Loss: 2.2377548217773438\n",
      "Starting epoch 100/500\n",
      "Epoch [100/500], Step [100/390], D Loss: 0.6695116758346558, G Loss: 2.7309751510620117\n",
      "Epoch [100/500], Step [200/390], D Loss: 0.9961981773376465, G Loss: 1.375955581665039\n",
      "Epoch [100/500], Step [300/390], D Loss: 1.0694040060043335, G Loss: 2.0957696437835693\n",
      "Starting epoch 101/500\n",
      "Epoch [101/500], Step [100/390], D Loss: 0.7250850200653076, G Loss: 2.878065586090088\n",
      "Epoch [101/500], Step [200/390], D Loss: 0.897688627243042, G Loss: 1.9382143020629883\n",
      "Epoch [101/500], Step [300/390], D Loss: 0.5568362474441528, G Loss: 2.7486376762390137\n",
      "Starting epoch 102/500\n",
      "Epoch [102/500], Step [100/390], D Loss: 0.881428599357605, G Loss: 2.6002652645111084\n",
      "Epoch [102/500], Step [200/390], D Loss: 0.45909595489501953, G Loss: 2.500892162322998\n",
      "Epoch [102/500], Step [300/390], D Loss: 0.6330890655517578, G Loss: 1.6753849983215332\n",
      "Starting epoch 103/500\n",
      "Epoch [103/500], Step [100/390], D Loss: 0.6702068448066711, G Loss: 3.3180460929870605\n",
      "Epoch [103/500], Step [200/390], D Loss: 0.6934340000152588, G Loss: 3.198284864425659\n",
      "Epoch [103/500], Step [300/390], D Loss: 0.8032814860343933, G Loss: 2.709282636642456\n",
      "Starting epoch 104/500\n",
      "Epoch [104/500], Step [100/390], D Loss: 0.9356644153594971, G Loss: 1.3250470161437988\n",
      "Epoch [104/500], Step [200/390], D Loss: 1.1954729557037354, G Loss: 1.5884547233581543\n",
      "Epoch [104/500], Step [300/390], D Loss: 1.0164979696273804, G Loss: 1.5053455829620361\n",
      "Starting epoch 105/500\n",
      "Epoch [105/500], Step [100/390], D Loss: 1.0371668338775635, G Loss: 1.703341007232666\n",
      "Epoch [105/500], Step [200/390], D Loss: 1.9098104238510132, G Loss: 2.0704944133758545\n",
      "Epoch [105/500], Step [300/390], D Loss: 0.6598204374313354, G Loss: 3.152420997619629\n",
      "Starting epoch 106/500\n",
      "Epoch [106/500], Step [100/390], D Loss: 1.013634443283081, G Loss: 1.6500576734542847\n",
      "Epoch [106/500], Step [200/390], D Loss: 0.8076022863388062, G Loss: 3.7660415172576904\n",
      "Epoch [106/500], Step [300/390], D Loss: 0.6122462749481201, G Loss: 2.632016181945801\n",
      "Starting epoch 107/500\n",
      "Epoch [107/500], Step [100/390], D Loss: 0.7386196851730347, G Loss: 2.182260036468506\n",
      "Epoch [107/500], Step [200/390], D Loss: 0.3350321352481842, G Loss: 2.98636531829834\n",
      "Epoch [107/500], Step [300/390], D Loss: 0.35618317127227783, G Loss: 3.382938861846924\n",
      "Starting epoch 108/500\n",
      "Epoch [108/500], Step [100/390], D Loss: 0.39723458886146545, G Loss: 3.2873222827911377\n",
      "Epoch [108/500], Step [200/390], D Loss: 1.2028712034225464, G Loss: 2.6774637699127197\n",
      "Epoch [108/500], Step [300/390], D Loss: 0.3172464370727539, G Loss: 1.9932459592819214\n",
      "Starting epoch 109/500\n",
      "Epoch [109/500], Step [100/390], D Loss: 0.6727765202522278, G Loss: 1.980129599571228\n",
      "Epoch [109/500], Step [200/390], D Loss: 0.9567639827728271, G Loss: 3.9283645153045654\n",
      "Epoch [109/500], Step [300/390], D Loss: 0.5628398060798645, G Loss: 1.9260179996490479\n",
      "Starting epoch 110/500\n",
      "Epoch [110/500], Step [100/390], D Loss: 0.12268344312906265, G Loss: 3.6983134746551514\n",
      "Epoch [110/500], Step [200/390], D Loss: 0.8437594771385193, G Loss: 2.6099741458892822\n",
      "Epoch [110/500], Step [300/390], D Loss: 0.37123024463653564, G Loss: 2.174821376800537\n",
      "Starting epoch 111/500\n",
      "Epoch [111/500], Step [100/390], D Loss: 1.065903902053833, G Loss: 2.74819016456604\n"
     ]
    }
   ],
   "source": [
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Optimizers\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting epoch {epoch+1}/{num_epochs}')\n",
    "    for i, (images, _) in enumerate(train_loader_cifar10):\n",
    "        real_images = images.to(device)\n",
    "        batch_size = real_images.size(0)\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        # --------- Train the Discriminator --------- #\n",
    "        d_optimizer.zero_grad()\n",
    "        outputs = discriminator(real_images)\n",
    "        d_real_loss = criterion(outputs, real_labels)\n",
    "        z = torch.randn(batch_size, latent_dim).to(device)\n",
    "        fake_images = generator(z)\n",
    "        outputs = discriminator(fake_images.detach())\n",
    "        d_fake_loss = criterion(outputs, fake_labels)\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # --------- Train the Generator --------- #\n",
    "        g_optimizer.zero_grad()\n",
    "        outputs = discriminator(fake_images)\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:  # More frequent logging\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader_cifar10)}], D Loss: {d_loss.item()}, G Loss: {g_loss.item()}')\n",
    "\n",
    "    # Save generated images every epoch\n",
    "    save_image(fake_images.data[:25], f'./data/cifar10/fake_image_{epoch+1:03d}.png', nrow=5, normalize=True)\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
