{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T02:20:52.147218789Z",
     "start_time": "2024-05-16T02:20:52.138152729Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "lr = 0.0002\n",
    "batch_size = 128\n",
    "image_size = 32 * 32  # CIFAR-10 images are 32x32\n",
    "num_epochs = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T02:20:56.432588675Z",
     "start_time": "2024-05-16T02:20:52.142676521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Verifying data loading...\n",
      "Batch shape: torch.Size([128, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "# CIFAR-10 dataset\n",
    "transform_cifar10 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training dataset\n",
    "train_dataset_cifar10 = datasets.CIFAR10(root='./data', train=True, transform=transform_cifar10, download=True)\n",
    "train_loader_cifar10 = DataLoader(dataset=train_dataset_cifar10, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# Load CIFAR-10 test dataset\n",
    "test_dataset_cifar10 = datasets.CIFAR10(root='./data', train=False, transform=transform_cifar10, download=True)\n",
    "test_loader_cifar10 = DataLoader(dataset=test_dataset_cifar10, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "# Move data to the specified device (optional)\n",
    "for images, labels in train_loader_cifar10:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    # Your training code here\n",
    "\n",
    "for images, labels in test_loader_cifar10:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "print(\"Verifying data loading...\")\n",
    "for images, labels in train_loader_cifar10:\n",
    "    print(f'Batch shape: {images.shape}')\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T02:20:56.436798478Z",
     "start_time": "2024-05-16T02:20:56.432851669Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.init_size = 8  # Initial size before upsampling\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, img_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.model(out)\n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.adv_layer = nn.Sequential(\n",
    "            nn.Linear(512 * 2 * 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.size(0), -1)  # Flatten the output\n",
    "        validity = self.adv_layer(out)\n",
    "        return validity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-05-16T02:20:56.435969680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Starting epoch 1/500\n",
      "Epoch [1/500], Step [100/390], D Loss: 0.3555479645729065, G Loss: 4.317956924438477\n",
      "Epoch [1/500], Step [200/390], D Loss: 0.8303543925285339, G Loss: 1.98268461227417\n",
      "Epoch [1/500], Step [300/390], D Loss: 0.9628589749336243, G Loss: 2.2876009941101074\n",
      "Starting epoch 2/500\n",
      "Epoch [2/500], Step [100/390], D Loss: 0.8369532823562622, G Loss: 1.5960098505020142\n",
      "Epoch [2/500], Step [200/390], D Loss: 0.9370321035385132, G Loss: 1.6010043621063232\n",
      "Epoch [2/500], Step [300/390], D Loss: 1.392399787902832, G Loss: 1.723888635635376\n",
      "Starting epoch 3/500\n",
      "Epoch [3/500], Step [100/390], D Loss: 1.3697881698608398, G Loss: 1.469482660293579\n",
      "Epoch [3/500], Step [200/390], D Loss: 1.1382144689559937, G Loss: 1.0763030052185059\n",
      "Epoch [3/500], Step [300/390], D Loss: 1.2574814558029175, G Loss: 0.7503430247306824\n",
      "Starting epoch 4/500\n",
      "Epoch [4/500], Step [100/390], D Loss: 1.7877590656280518, G Loss: 0.9839044213294983\n",
      "Epoch [4/500], Step [200/390], D Loss: 1.0824155807495117, G Loss: 0.9392586946487427\n",
      "Epoch [4/500], Step [300/390], D Loss: 1.1833765506744385, G Loss: 0.9216644167900085\n",
      "Starting epoch 5/500\n",
      "Epoch [5/500], Step [100/390], D Loss: 0.5991721153259277, G Loss: 2.6420223712921143\n",
      "Epoch [5/500], Step [200/390], D Loss: 1.900516390800476, G Loss: 1.0565910339355469\n",
      "Epoch [5/500], Step [300/390], D Loss: 0.6452029347419739, G Loss: 1.900869607925415\n",
      "Starting epoch 6/500\n",
      "Epoch [6/500], Step [100/390], D Loss: 1.00863778591156, G Loss: 1.2705023288726807\n",
      "Epoch [6/500], Step [200/390], D Loss: 1.1463706493377686, G Loss: 1.043007493019104\n",
      "Epoch [6/500], Step [300/390], D Loss: 1.0412631034851074, G Loss: 1.2080543041229248\n",
      "Starting epoch 7/500\n",
      "Epoch [7/500], Step [100/390], D Loss: 1.0610873699188232, G Loss: 1.4517085552215576\n",
      "Epoch [7/500], Step [200/390], D Loss: 0.907781720161438, G Loss: 1.1234519481658936\n",
      "Epoch [7/500], Step [300/390], D Loss: 0.8903230428695679, G Loss: 1.219773769378662\n",
      "Starting epoch 8/500\n",
      "Epoch [8/500], Step [100/390], D Loss: 1.2279056310653687, G Loss: 0.9787030816078186\n",
      "Epoch [8/500], Step [200/390], D Loss: 0.858059823513031, G Loss: 1.4289458990097046\n",
      "Epoch [8/500], Step [300/390], D Loss: 1.162780523300171, G Loss: 1.440356969833374\n",
      "Starting epoch 9/500\n",
      "Epoch [9/500], Step [100/390], D Loss: 1.0256388187408447, G Loss: 2.002612590789795\n",
      "Epoch [9/500], Step [200/390], D Loss: 1.5069036483764648, G Loss: 1.9198110103607178\n",
      "Epoch [9/500], Step [300/390], D Loss: 0.742281973361969, G Loss: 2.0287013053894043\n",
      "Starting epoch 10/500\n",
      "Epoch [10/500], Step [100/390], D Loss: 1.174271821975708, G Loss: 1.1565215587615967\n",
      "Epoch [10/500], Step [200/390], D Loss: 0.9519709944725037, G Loss: 1.303840160369873\n",
      "Epoch [10/500], Step [300/390], D Loss: 1.220651626586914, G Loss: 1.220403790473938\n",
      "Starting epoch 11/500\n",
      "Epoch [11/500], Step [100/390], D Loss: 1.6856080293655396, G Loss: 0.6988266110420227\n",
      "Epoch [11/500], Step [200/390], D Loss: 1.1860013008117676, G Loss: 0.9671339988708496\n",
      "Epoch [11/500], Step [300/390], D Loss: 0.8058177828788757, G Loss: 0.9868578910827637\n",
      "Starting epoch 12/500\n",
      "Epoch [12/500], Step [100/390], D Loss: 0.8544011116027832, G Loss: 1.4326717853546143\n",
      "Epoch [12/500], Step [200/390], D Loss: 1.203821063041687, G Loss: 0.9414066672325134\n",
      "Epoch [12/500], Step [300/390], D Loss: 1.0882484912872314, G Loss: 0.9907917380332947\n",
      "Starting epoch 13/500\n",
      "Epoch [13/500], Step [100/390], D Loss: 1.2960801124572754, G Loss: 1.3699710369110107\n",
      "Epoch [13/500], Step [200/390], D Loss: 0.8039054870605469, G Loss: 2.0497031211853027\n",
      "Epoch [13/500], Step [300/390], D Loss: 1.1550192832946777, G Loss: 1.2738137245178223\n",
      "Starting epoch 14/500\n",
      "Epoch [14/500], Step [100/390], D Loss: 1.4962596893310547, G Loss: 0.8438830971717834\n",
      "Epoch [14/500], Step [200/390], D Loss: 1.1365196704864502, G Loss: 0.9314039945602417\n",
      "Epoch [14/500], Step [300/390], D Loss: 1.0348570346832275, G Loss: 1.1549432277679443\n",
      "Starting epoch 15/500\n",
      "Epoch [15/500], Step [100/390], D Loss: 1.3452471494674683, G Loss: 1.3258414268493652\n",
      "Epoch [15/500], Step [200/390], D Loss: 1.1262954473495483, G Loss: 0.959618091583252\n",
      "Epoch [15/500], Step [300/390], D Loss: 1.0238540172576904, G Loss: 0.9820971488952637\n",
      "Starting epoch 16/500\n",
      "Epoch [16/500], Step [100/390], D Loss: 1.181077241897583, G Loss: 1.2155578136444092\n",
      "Epoch [16/500], Step [200/390], D Loss: 1.142409086227417, G Loss: 1.0471396446228027\n",
      "Epoch [16/500], Step [300/390], D Loss: 1.2216176986694336, G Loss: 1.0683943033218384\n",
      "Starting epoch 17/500\n",
      "Epoch [17/500], Step [100/390], D Loss: 1.1449298858642578, G Loss: 1.122744083404541\n",
      "Epoch [17/500], Step [200/390], D Loss: 1.0529931783676147, G Loss: 1.2523057460784912\n",
      "Epoch [17/500], Step [300/390], D Loss: 0.7741445302963257, G Loss: 1.3068690299987793\n",
      "Starting epoch 18/500\n",
      "Epoch [18/500], Step [100/390], D Loss: 1.081428050994873, G Loss: 1.0121440887451172\n",
      "Epoch [18/500], Step [200/390], D Loss: 1.1189316511154175, G Loss: 1.6601651906967163\n",
      "Epoch [18/500], Step [300/390], D Loss: 0.5803921222686768, G Loss: 1.5368324518203735\n",
      "Starting epoch 19/500\n",
      "Epoch [19/500], Step [100/390], D Loss: 1.0103328227996826, G Loss: 1.2083232402801514\n",
      "Epoch [19/500], Step [200/390], D Loss: 0.9928522706031799, G Loss: 1.2619627714157104\n",
      "Epoch [19/500], Step [300/390], D Loss: 1.0233999490737915, G Loss: 1.219186544418335\n",
      "Starting epoch 20/500\n",
      "Epoch [20/500], Step [100/390], D Loss: 0.9585875272750854, G Loss: 1.4590153694152832\n",
      "Epoch [20/500], Step [200/390], D Loss: 0.7498663067817688, G Loss: 1.6534550189971924\n",
      "Epoch [20/500], Step [300/390], D Loss: 0.7089769840240479, G Loss: 1.6363182067871094\n",
      "Starting epoch 21/500\n",
      "Epoch [21/500], Step [100/390], D Loss: 0.9670511484146118, G Loss: 1.1969228982925415\n",
      "Epoch [21/500], Step [200/390], D Loss: 0.8765769004821777, G Loss: 1.8509571552276611\n",
      "Epoch [21/500], Step [300/390], D Loss: 0.8050488829612732, G Loss: 1.3027687072753906\n",
      "Starting epoch 22/500\n",
      "Epoch [22/500], Step [100/390], D Loss: 0.6592846512794495, G Loss: 1.6349461078643799\n",
      "Epoch [22/500], Step [200/390], D Loss: 0.8878782391548157, G Loss: 1.315805196762085\n",
      "Epoch [22/500], Step [300/390], D Loss: 0.8271414041519165, G Loss: 2.070038080215454\n",
      "Starting epoch 23/500\n",
      "Epoch [23/500], Step [100/390], D Loss: 0.7811771631240845, G Loss: 2.082139730453491\n",
      "Epoch [23/500], Step [200/390], D Loss: 1.0931899547576904, G Loss: 1.5272984504699707\n",
      "Epoch [23/500], Step [300/390], D Loss: 0.7203340530395508, G Loss: 1.7587816715240479\n",
      "Starting epoch 24/500\n",
      "Epoch [24/500], Step [100/390], D Loss: 0.8462095260620117, G Loss: 1.5289498567581177\n",
      "Epoch [24/500], Step [200/390], D Loss: 0.8378076553344727, G Loss: 1.323920726776123\n",
      "Epoch [24/500], Step [300/390], D Loss: 0.9389784336090088, G Loss: 1.3703867197036743\n",
      "Starting epoch 25/500\n",
      "Epoch [25/500], Step [100/390], D Loss: 0.6266862154006958, G Loss: 1.9061607122421265\n",
      "Epoch [25/500], Step [200/390], D Loss: 0.7215390801429749, G Loss: 1.8247158527374268\n",
      "Epoch [25/500], Step [300/390], D Loss: 0.9842721223831177, G Loss: 1.589855432510376\n",
      "Starting epoch 26/500\n",
      "Epoch [26/500], Step [100/390], D Loss: 0.9087192416191101, G Loss: 1.2310693264007568\n",
      "Epoch [26/500], Step [200/390], D Loss: 0.7850662469863892, G Loss: 1.78423273563385\n",
      "Epoch [26/500], Step [300/390], D Loss: 0.9297893643379211, G Loss: 1.4403373003005981\n",
      "Starting epoch 27/500\n",
      "Epoch [27/500], Step [100/390], D Loss: 0.6127077341079712, G Loss: 1.9249756336212158\n",
      "Epoch [27/500], Step [200/390], D Loss: 0.7804421186447144, G Loss: 2.379049301147461\n",
      "Epoch [27/500], Step [300/390], D Loss: 0.8963034152984619, G Loss: 1.7581048011779785\n",
      "Starting epoch 28/500\n",
      "Epoch [28/500], Step [100/390], D Loss: 0.7762043476104736, G Loss: 1.9164159297943115\n",
      "Epoch [28/500], Step [200/390], D Loss: 0.868091344833374, G Loss: 1.6779110431671143\n",
      "Epoch [28/500], Step [300/390], D Loss: 1.2263563871383667, G Loss: 1.7691115140914917\n",
      "Starting epoch 29/500\n",
      "Epoch [29/500], Step [100/390], D Loss: 0.5142028331756592, G Loss: 2.2211906909942627\n",
      "Epoch [29/500], Step [200/390], D Loss: 0.828954815864563, G Loss: 1.5161583423614502\n",
      "Epoch [29/500], Step [300/390], D Loss: 0.7520304918289185, G Loss: 1.3954167366027832\n",
      "Starting epoch 30/500\n",
      "Epoch [30/500], Step [100/390], D Loss: 0.7154198884963989, G Loss: 2.106839179992676\n",
      "Epoch [30/500], Step [200/390], D Loss: 0.5558285713195801, G Loss: 2.509521007537842\n",
      "Epoch [30/500], Step [300/390], D Loss: 0.9084300398826599, G Loss: 1.6157326698303223\n",
      "Starting epoch 31/500\n",
      "Epoch [31/500], Step [100/390], D Loss: 0.7785196900367737, G Loss: 1.8008544445037842\n",
      "Epoch [31/500], Step [200/390], D Loss: 0.9562082290649414, G Loss: 1.323663353919983\n",
      "Epoch [31/500], Step [300/390], D Loss: 0.9339730739593506, G Loss: 1.6341365575790405\n",
      "Starting epoch 32/500\n",
      "Epoch [32/500], Step [100/390], D Loss: 0.7771424055099487, G Loss: 1.7265044450759888\n",
      "Epoch [32/500], Step [200/390], D Loss: 0.915067732334137, G Loss: 1.5507359504699707\n",
      "Epoch [32/500], Step [300/390], D Loss: 1.5792146921157837, G Loss: 1.5027568340301514\n",
      "Starting epoch 33/500\n",
      "Epoch [33/500], Step [100/390], D Loss: 0.627815842628479, G Loss: 2.5834288597106934\n",
      "Epoch [33/500], Step [200/390], D Loss: 1.0653961896896362, G Loss: 1.6927087306976318\n",
      "Epoch [33/500], Step [300/390], D Loss: 0.942695677280426, G Loss: 1.5141441822052002\n",
      "Starting epoch 34/500\n",
      "Epoch [34/500], Step [100/390], D Loss: 0.5734521150588989, G Loss: 2.002505302429199\n",
      "Epoch [34/500], Step [200/390], D Loss: 0.8877859115600586, G Loss: 1.7770419120788574\n",
      "Epoch [34/500], Step [300/390], D Loss: 0.6861835718154907, G Loss: 2.081531047821045\n",
      "Starting epoch 35/500\n",
      "Epoch [35/500], Step [100/390], D Loss: 0.7949004173278809, G Loss: 1.7349311113357544\n",
      "Epoch [35/500], Step [200/390], D Loss: 0.8165866732597351, G Loss: 1.5216853618621826\n",
      "Epoch [35/500], Step [300/390], D Loss: 1.1512658596038818, G Loss: 1.4208084344863892\n",
      "Starting epoch 36/500\n",
      "Epoch [36/500], Step [100/390], D Loss: 0.9915747046470642, G Loss: 2.10427188873291\n",
      "Epoch [36/500], Step [200/390], D Loss: 0.997489333152771, G Loss: 1.2136784791946411\n",
      "Epoch [36/500], Step [300/390], D Loss: 0.9115249514579773, G Loss: 1.7375950813293457\n",
      "Starting epoch 37/500\n",
      "Epoch [37/500], Step [100/390], D Loss: 0.934653103351593, G Loss: 1.586334466934204\n",
      "Epoch [37/500], Step [200/390], D Loss: 1.173298954963684, G Loss: 1.6622494459152222\n",
      "Epoch [37/500], Step [300/390], D Loss: 0.9044746160507202, G Loss: 1.3636729717254639\n",
      "Starting epoch 38/500\n",
      "Epoch [38/500], Step [100/390], D Loss: 0.9696555137634277, G Loss: 1.375120997428894\n",
      "Epoch [38/500], Step [200/390], D Loss: 1.1919200420379639, G Loss: 1.9653359651565552\n",
      "Epoch [38/500], Step [300/390], D Loss: 0.8508859276771545, G Loss: 1.3899940252304077\n",
      "Starting epoch 39/500\n",
      "Epoch [39/500], Step [100/390], D Loss: 0.9973602294921875, G Loss: 1.486270546913147\n",
      "Epoch [39/500], Step [200/390], D Loss: 0.895592987537384, G Loss: 1.5094205141067505\n",
      "Epoch [39/500], Step [300/390], D Loss: 0.9050277471542358, G Loss: 1.4630039930343628\n",
      "Starting epoch 40/500\n",
      "Epoch [40/500], Step [100/390], D Loss: 0.931382417678833, G Loss: 1.414505958557129\n",
      "Epoch [40/500], Step [200/390], D Loss: 0.7177303433418274, G Loss: 1.9228399991989136\n",
      "Epoch [40/500], Step [300/390], D Loss: 1.0581351518630981, G Loss: 1.302382469177246\n",
      "Starting epoch 41/500\n",
      "Epoch [41/500], Step [100/390], D Loss: 0.7208276987075806, G Loss: 1.793708086013794\n",
      "Epoch [41/500], Step [200/390], D Loss: 0.8252671957015991, G Loss: 1.8221514225006104\n",
      "Epoch [41/500], Step [300/390], D Loss: 1.0517640113830566, G Loss: 1.26638662815094\n",
      "Starting epoch 42/500\n",
      "Epoch [42/500], Step [100/390], D Loss: 1.1382155418395996, G Loss: 1.0645712614059448\n",
      "Epoch [42/500], Step [200/390], D Loss: 0.8982245326042175, G Loss: 1.3177926540374756\n",
      "Epoch [42/500], Step [300/390], D Loss: 0.9537292718887329, G Loss: 1.3597099781036377\n",
      "Starting epoch 43/500\n",
      "Epoch [43/500], Step [100/390], D Loss: 1.036487102508545, G Loss: 1.62607741355896\n",
      "Epoch [43/500], Step [200/390], D Loss: 0.9579585790634155, G Loss: 1.4256770610809326\n",
      "Epoch [43/500], Step [300/390], D Loss: 1.0383933782577515, G Loss: 1.2821476459503174\n",
      "Starting epoch 44/500\n",
      "Epoch [44/500], Step [100/390], D Loss: 0.8344547748565674, G Loss: 1.870436191558838\n",
      "Epoch [44/500], Step [200/390], D Loss: 1.0630762577056885, G Loss: 1.5403543710708618\n",
      "Epoch [44/500], Step [300/390], D Loss: 0.9678021669387817, G Loss: 1.2983766794204712\n",
      "Starting epoch 45/500\n",
      "Epoch [45/500], Step [100/390], D Loss: 1.0415092706680298, G Loss: 1.4620165824890137\n",
      "Epoch [45/500], Step [200/390], D Loss: 0.8815932273864746, G Loss: 1.5381019115447998\n",
      "Epoch [45/500], Step [300/390], D Loss: 0.9052720665931702, G Loss: 1.2531262636184692\n",
      "Starting epoch 46/500\n",
      "Epoch [46/500], Step [100/390], D Loss: 1.0011019706726074, G Loss: 1.647976040840149\n",
      "Epoch [46/500], Step [200/390], D Loss: 0.780983030796051, G Loss: 1.6353657245635986\n",
      "Epoch [46/500], Step [300/390], D Loss: 1.0603736639022827, G Loss: 1.3118774890899658\n",
      "Starting epoch 47/500\n",
      "Epoch [47/500], Step [100/390], D Loss: 0.9915481209754944, G Loss: 1.37679123878479\n",
      "Epoch [47/500], Step [200/390], D Loss: 0.859496533870697, G Loss: 1.426673173904419\n",
      "Epoch [47/500], Step [300/390], D Loss: 1.1324459314346313, G Loss: 1.4421577453613281\n",
      "Starting epoch 48/500\n",
      "Epoch [48/500], Step [100/390], D Loss: 0.9250714182853699, G Loss: 1.4403071403503418\n",
      "Epoch [48/500], Step [200/390], D Loss: 1.024816632270813, G Loss: 1.22391676902771\n",
      "Epoch [48/500], Step [300/390], D Loss: 1.089900016784668, G Loss: 1.3629804849624634\n",
      "Starting epoch 49/500\n",
      "Epoch [49/500], Step [100/390], D Loss: 0.9898225665092468, G Loss: 1.5933151245117188\n",
      "Epoch [49/500], Step [200/390], D Loss: 1.199439525604248, G Loss: 1.3407338857650757\n",
      "Epoch [49/500], Step [300/390], D Loss: 1.0079083442687988, G Loss: 1.2415053844451904\n",
      "Starting epoch 50/500\n",
      "Epoch [50/500], Step [100/390], D Loss: 0.8569451570510864, G Loss: 1.394209623336792\n",
      "Epoch [50/500], Step [200/390], D Loss: 0.9891467094421387, G Loss: 1.2469648122787476\n",
      "Epoch [50/500], Step [300/390], D Loss: 1.2841343879699707, G Loss: 1.2984765768051147\n",
      "Starting epoch 51/500\n",
      "Epoch [51/500], Step [100/390], D Loss: 0.8429409265518188, G Loss: 1.4757201671600342\n",
      "Epoch [51/500], Step [200/390], D Loss: 1.010050654411316, G Loss: 1.219864845275879\n",
      "Epoch [51/500], Step [300/390], D Loss: 1.120906114578247, G Loss: 1.235898733139038\n",
      "Starting epoch 52/500\n",
      "Epoch [52/500], Step [100/390], D Loss: 1.01285719871521, G Loss: 1.1410980224609375\n",
      "Epoch [52/500], Step [200/390], D Loss: 1.21714186668396, G Loss: 1.1800882816314697\n",
      "Epoch [52/500], Step [300/390], D Loss: 1.1516790390014648, G Loss: 1.351585865020752\n",
      "Starting epoch 53/500\n",
      "Epoch [53/500], Step [100/390], D Loss: 1.148688554763794, G Loss: 1.0639667510986328\n",
      "Epoch [53/500], Step [200/390], D Loss: 0.8274823427200317, G Loss: 1.5720475912094116\n",
      "Epoch [53/500], Step [300/390], D Loss: 0.9799087047576904, G Loss: 1.4203522205352783\n",
      "Starting epoch 54/500\n",
      "Epoch [54/500], Step [100/390], D Loss: 0.9508504867553711, G Loss: 1.2036643028259277\n",
      "Epoch [54/500], Step [200/390], D Loss: 0.9457271695137024, G Loss: 1.6399842500686646\n",
      "Epoch [54/500], Step [300/390], D Loss: 1.096073865890503, G Loss: 1.236663579940796\n",
      "Starting epoch 55/500\n",
      "Epoch [55/500], Step [100/390], D Loss: 0.8561407923698425, G Loss: 1.3211066722869873\n",
      "Epoch [55/500], Step [200/390], D Loss: 0.9383167028427124, G Loss: 1.372686743736267\n",
      "Epoch [55/500], Step [300/390], D Loss: 0.9714921116828918, G Loss: 1.1173045635223389\n",
      "Starting epoch 56/500\n",
      "Epoch [56/500], Step [100/390], D Loss: 1.0040980577468872, G Loss: 1.54618239402771\n",
      "Epoch [56/500], Step [200/390], D Loss: 0.9258959889411926, G Loss: 1.3111170530319214\n",
      "Epoch [56/500], Step [300/390], D Loss: 0.907169759273529, G Loss: 1.4266976118087769\n",
      "Starting epoch 57/500\n",
      "Epoch [57/500], Step [100/390], D Loss: 0.9996407628059387, G Loss: 1.3494527339935303\n",
      "Epoch [57/500], Step [200/390], D Loss: 1.007795810699463, G Loss: 1.200846552848816\n",
      "Epoch [57/500], Step [300/390], D Loss: 0.9873960018157959, G Loss: 1.308767557144165\n",
      "Starting epoch 58/500\n",
      "Epoch [58/500], Step [100/390], D Loss: 0.7415661811828613, G Loss: 1.732269287109375\n",
      "Epoch [58/500], Step [200/390], D Loss: 1.048391342163086, G Loss: 1.2086515426635742\n",
      "Epoch [58/500], Step [300/390], D Loss: 0.9957752227783203, G Loss: 1.4632251262664795\n",
      "Starting epoch 59/500\n",
      "Epoch [59/500], Step [100/390], D Loss: 0.96463942527771, G Loss: 1.4707088470458984\n",
      "Epoch [59/500], Step [200/390], D Loss: 0.9957458972930908, G Loss: 1.2791398763656616\n",
      "Epoch [59/500], Step [300/390], D Loss: 0.9301342964172363, G Loss: 1.284411072731018\n",
      "Starting epoch 60/500\n",
      "Epoch [60/500], Step [100/390], D Loss: 0.9730796813964844, G Loss: 1.3883260488510132\n",
      "Epoch [60/500], Step [200/390], D Loss: 0.9886499643325806, G Loss: 1.327559232711792\n",
      "Epoch [60/500], Step [300/390], D Loss: 0.9590351581573486, G Loss: 1.3902626037597656\n",
      "Starting epoch 61/500\n",
      "Epoch [61/500], Step [100/390], D Loss: 0.9521377682685852, G Loss: 1.4857306480407715\n",
      "Epoch [61/500], Step [200/390], D Loss: 0.9615098834037781, G Loss: 1.5481185913085938\n",
      "Epoch [61/500], Step [300/390], D Loss: 1.0301496982574463, G Loss: 1.2372796535491943\n",
      "Starting epoch 62/500\n",
      "Epoch [62/500], Step [100/390], D Loss: 0.9512419700622559, G Loss: 1.1777464151382446\n",
      "Epoch [62/500], Step [200/390], D Loss: 0.9623605012893677, G Loss: 1.338072657585144\n",
      "Epoch [62/500], Step [300/390], D Loss: 0.9157949686050415, G Loss: 1.3030180931091309\n",
      "Starting epoch 63/500\n",
      "Epoch [63/500], Step [100/390], D Loss: 1.0720778703689575, G Loss: 1.2289977073669434\n",
      "Epoch [63/500], Step [200/390], D Loss: 0.8661566972732544, G Loss: 1.3598029613494873\n",
      "Epoch [63/500], Step [300/390], D Loss: 0.9231754541397095, G Loss: 1.504171371459961\n",
      "Starting epoch 64/500\n",
      "Epoch [64/500], Step [100/390], D Loss: 1.0789566040039062, G Loss: 1.279362440109253\n",
      "Epoch [64/500], Step [200/390], D Loss: 0.9542626142501831, G Loss: 1.2955214977264404\n",
      "Epoch [64/500], Step [300/390], D Loss: 0.7907218933105469, G Loss: 1.2486985921859741\n",
      "Starting epoch 65/500\n",
      "Epoch [65/500], Step [100/390], D Loss: 1.072438359260559, G Loss: 1.200723648071289\n",
      "Epoch [65/500], Step [200/390], D Loss: 0.981323778629303, G Loss: 1.3953478336334229\n",
      "Epoch [65/500], Step [300/390], D Loss: 0.8955298662185669, G Loss: 1.2934670448303223\n",
      "Starting epoch 66/500\n",
      "Epoch [66/500], Step [100/390], D Loss: 0.955602765083313, G Loss: 1.5179383754730225\n",
      "Epoch [66/500], Step [200/390], D Loss: 0.920719563961029, G Loss: 1.4214394092559814\n",
      "Epoch [66/500], Step [300/390], D Loss: 0.9161686897277832, G Loss: 1.3375346660614014\n",
      "Starting epoch 67/500\n",
      "Epoch [67/500], Step [100/390], D Loss: 0.9044985175132751, G Loss: 1.3163199424743652\n",
      "Epoch [67/500], Step [200/390], D Loss: 1.0046195983886719, G Loss: 1.1237250566482544\n",
      "Epoch [67/500], Step [300/390], D Loss: 0.9399263858795166, G Loss: 1.334617257118225\n",
      "Starting epoch 68/500\n",
      "Epoch [68/500], Step [100/390], D Loss: 1.1383144855499268, G Loss: 1.21036958694458\n",
      "Epoch [68/500], Step [200/390], D Loss: 0.9184901714324951, G Loss: 1.332970142364502\n",
      "Epoch [68/500], Step [300/390], D Loss: 0.9983258843421936, G Loss: 1.3629816770553589\n",
      "Starting epoch 69/500\n",
      "Epoch [69/500], Step [100/390], D Loss: 0.9327417612075806, G Loss: 1.2551052570343018\n",
      "Epoch [69/500], Step [200/390], D Loss: 0.9584119319915771, G Loss: 1.3739795684814453\n",
      "Epoch [69/500], Step [300/390], D Loss: 1.0698630809783936, G Loss: 1.271240234375\n",
      "Starting epoch 70/500\n",
      "Epoch [70/500], Step [100/390], D Loss: 1.1032366752624512, G Loss: 1.1964291334152222\n",
      "Epoch [70/500], Step [200/390], D Loss: 0.9892078638076782, G Loss: 1.28016996383667\n",
      "Epoch [70/500], Step [300/390], D Loss: 1.0635724067687988, G Loss: 1.3551205396652222\n",
      "Starting epoch 71/500\n",
      "Epoch [71/500], Step [100/390], D Loss: 0.990242600440979, G Loss: 1.3579381704330444\n",
      "Epoch [71/500], Step [200/390], D Loss: 1.017040491104126, G Loss: 1.2236196994781494\n",
      "Epoch [71/500], Step [300/390], D Loss: 0.9989041090011597, G Loss: 1.3437702655792236\n",
      "Starting epoch 72/500\n",
      "Epoch [72/500], Step [100/390], D Loss: 0.9887052774429321, G Loss: 1.3658726215362549\n",
      "Epoch [72/500], Step [200/390], D Loss: 0.9558068513870239, G Loss: 1.4250459671020508\n",
      "Epoch [72/500], Step [300/390], D Loss: 1.075544834136963, G Loss: 1.3183436393737793\n",
      "Starting epoch 73/500\n",
      "Epoch [73/500], Step [100/390], D Loss: 0.9866889119148254, G Loss: 1.4284207820892334\n",
      "Epoch [73/500], Step [200/390], D Loss: 1.0464444160461426, G Loss: 1.2627017498016357\n",
      "Epoch [73/500], Step [300/390], D Loss: 1.0417068004608154, G Loss: 1.4272481203079224\n",
      "Starting epoch 74/500\n",
      "Epoch [74/500], Step [100/390], D Loss: 0.9369003176689148, G Loss: 1.4059439897537231\n",
      "Epoch [74/500], Step [200/390], D Loss: 1.0590732097625732, G Loss: 1.2714208364486694\n",
      "Epoch [74/500], Step [300/390], D Loss: 0.9213791489601135, G Loss: 1.1783523559570312\n",
      "Starting epoch 75/500\n",
      "Epoch [75/500], Step [100/390], D Loss: 0.9970607757568359, G Loss: 1.249468445777893\n",
      "Epoch [75/500], Step [200/390], D Loss: 1.0467256307601929, G Loss: 1.334936261177063\n",
      "Epoch [75/500], Step [300/390], D Loss: 0.9600088000297546, G Loss: 1.4578979015350342\n",
      "Starting epoch 76/500\n",
      "Epoch [76/500], Step [100/390], D Loss: 1.2797551155090332, G Loss: 1.2269583940505981\n",
      "Epoch [76/500], Step [200/390], D Loss: 0.9473438858985901, G Loss: 1.2161169052124023\n",
      "Epoch [76/500], Step [300/390], D Loss: 1.1557574272155762, G Loss: 1.0742374658584595\n",
      "Starting epoch 77/500\n",
      "Epoch [77/500], Step [100/390], D Loss: 1.0096713304519653, G Loss: 1.2472999095916748\n",
      "Epoch [77/500], Step [200/390], D Loss: 0.9962776303291321, G Loss: 1.2681984901428223\n",
      "Epoch [77/500], Step [300/390], D Loss: 0.9249836802482605, G Loss: 1.345019817352295\n",
      "Starting epoch 78/500\n",
      "Epoch [78/500], Step [100/390], D Loss: 1.1798381805419922, G Loss: 1.413663387298584\n",
      "Epoch [78/500], Step [200/390], D Loss: 1.138036847114563, G Loss: 1.5210490226745605\n",
      "Epoch [78/500], Step [300/390], D Loss: 1.0318167209625244, G Loss: 1.3409671783447266\n",
      "Starting epoch 79/500\n",
      "Epoch [79/500], Step [100/390], D Loss: 1.1364669799804688, G Loss: 1.3086411952972412\n",
      "Epoch [79/500], Step [200/390], D Loss: 1.1981608867645264, G Loss: 1.370339035987854\n",
      "Epoch [79/500], Step [300/390], D Loss: 1.0168306827545166, G Loss: 1.4664554595947266\n",
      "Starting epoch 80/500\n",
      "Epoch [80/500], Step [100/390], D Loss: 1.12923002243042, G Loss: 1.3298628330230713\n",
      "Epoch [80/500], Step [200/390], D Loss: 1.0038419961929321, G Loss: 1.4674843549728394\n",
      "Epoch [80/500], Step [300/390], D Loss: 1.0594604015350342, G Loss: 1.2323462963104248\n",
      "Starting epoch 81/500\n",
      "Epoch [81/500], Step [100/390], D Loss: 0.9803693294525146, G Loss: 1.4833877086639404\n",
      "Epoch [81/500], Step [200/390], D Loss: 1.0479036569595337, G Loss: 1.3601351976394653\n",
      "Epoch [81/500], Step [300/390], D Loss: 1.1832205057144165, G Loss: 1.1885627508163452\n",
      "Starting epoch 82/500\n",
      "Epoch [82/500], Step [100/390], D Loss: 1.0734789371490479, G Loss: 1.2798874378204346\n",
      "Epoch [82/500], Step [200/390], D Loss: 0.9838294982910156, G Loss: 1.3843764066696167\n",
      "Epoch [82/500], Step [300/390], D Loss: 0.955206573009491, G Loss: 1.3153784275054932\n",
      "Starting epoch 83/500\n",
      "Epoch [83/500], Step [100/390], D Loss: 0.9239400625228882, G Loss: 1.255293607711792\n",
      "Epoch [83/500], Step [200/390], D Loss: 1.0693632364273071, G Loss: 1.642609715461731\n",
      "Epoch [83/500], Step [300/390], D Loss: 1.0213671922683716, G Loss: 1.312699794769287\n",
      "Starting epoch 84/500\n",
      "Epoch [84/500], Step [100/390], D Loss: 1.061523675918579, G Loss: 1.2681124210357666\n",
      "Epoch [84/500], Step [200/390], D Loss: 1.0098979473114014, G Loss: 1.338537573814392\n",
      "Epoch [84/500], Step [300/390], D Loss: 1.0218862295150757, G Loss: 1.272650957107544\n",
      "Starting epoch 85/500\n",
      "Epoch [85/500], Step [100/390], D Loss: 1.0653626918792725, G Loss: 1.28107488155365\n",
      "Epoch [85/500], Step [200/390], D Loss: 1.0952502489089966, G Loss: 1.389098882675171\n",
      "Epoch [85/500], Step [300/390], D Loss: 0.9449341297149658, G Loss: 1.5133390426635742\n",
      "Starting epoch 86/500\n",
      "Epoch [86/500], Step [100/390], D Loss: 0.9201349020004272, G Loss: 1.2901400327682495\n",
      "Epoch [86/500], Step [200/390], D Loss: 1.1077548265457153, G Loss: 1.400031328201294\n",
      "Epoch [86/500], Step [300/390], D Loss: 0.9149433374404907, G Loss: 1.531482458114624\n",
      "Starting epoch 87/500\n",
      "Epoch [87/500], Step [100/390], D Loss: 1.1508021354675293, G Loss: 1.2278199195861816\n",
      "Epoch [87/500], Step [200/390], D Loss: 0.9308289885520935, G Loss: 1.4343526363372803\n",
      "Epoch [87/500], Step [300/390], D Loss: 1.0142630338668823, G Loss: 1.1978955268859863\n",
      "Starting epoch 88/500\n",
      "Epoch [88/500], Step [100/390], D Loss: 1.1262754201889038, G Loss: 1.2399239540100098\n",
      "Epoch [88/500], Step [200/390], D Loss: 0.9781903624534607, G Loss: 1.2874159812927246\n",
      "Epoch [88/500], Step [300/390], D Loss: 0.9758138656616211, G Loss: 1.1765925884246826\n",
      "Starting epoch 89/500\n",
      "Epoch [89/500], Step [100/390], D Loss: 0.9349758625030518, G Loss: 1.4297548532485962\n",
      "Epoch [89/500], Step [200/390], D Loss: 0.9551776051521301, G Loss: 1.2673919200897217\n",
      "Epoch [89/500], Step [300/390], D Loss: 1.048107624053955, G Loss: 1.404611587524414\n",
      "Starting epoch 90/500\n",
      "Epoch [90/500], Step [100/390], D Loss: 1.0882620811462402, G Loss: 1.1876274347305298\n",
      "Epoch [90/500], Step [200/390], D Loss: 1.02731454372406, G Loss: 1.4419002532958984\n",
      "Epoch [90/500], Step [300/390], D Loss: 0.9892216920852661, G Loss: 1.2132363319396973\n",
      "Starting epoch 91/500\n",
      "Epoch [91/500], Step [100/390], D Loss: 1.0510047674179077, G Loss: 1.2935500144958496\n",
      "Epoch [91/500], Step [200/390], D Loss: 1.02756929397583, G Loss: 1.3522394895553589\n",
      "Epoch [91/500], Step [300/390], D Loss: 0.9479197263717651, G Loss: 1.5587518215179443\n",
      "Starting epoch 92/500\n",
      "Epoch [92/500], Step [100/390], D Loss: 0.9646193385124207, G Loss: 1.339799404144287\n",
      "Epoch [92/500], Step [200/390], D Loss: 1.1446555852890015, G Loss: 1.2689939737319946\n",
      "Epoch [92/500], Step [300/390], D Loss: 0.7543221712112427, G Loss: 1.9172087907791138\n",
      "Starting epoch 93/500\n",
      "Epoch [93/500], Step [100/390], D Loss: 1.0325851440429688, G Loss: 1.067032814025879\n",
      "Epoch [93/500], Step [200/390], D Loss: 1.0492345094680786, G Loss: 1.1690442562103271\n",
      "Epoch [93/500], Step [300/390], D Loss: 1.0137434005737305, G Loss: 1.2396540641784668\n",
      "Starting epoch 94/500\n",
      "Epoch [94/500], Step [100/390], D Loss: 1.0240674018859863, G Loss: 1.232711672782898\n",
      "Epoch [94/500], Step [200/390], D Loss: 0.9773219227790833, G Loss: 1.2431923151016235\n",
      "Epoch [94/500], Step [300/390], D Loss: 1.0356261730194092, G Loss: 1.2837574481964111\n",
      "Starting epoch 95/500\n",
      "Epoch [95/500], Step [100/390], D Loss: 0.9309835433959961, G Loss: 1.39412260055542\n",
      "Epoch [95/500], Step [200/390], D Loss: 1.0084456205368042, G Loss: 1.3008124828338623\n",
      "Epoch [95/500], Step [300/390], D Loss: 0.9842320084571838, G Loss: 1.1984540224075317\n",
      "Starting epoch 96/500\n",
      "Epoch [96/500], Step [100/390], D Loss: 0.9900861978530884, G Loss: 1.20588219165802\n",
      "Epoch [96/500], Step [200/390], D Loss: 0.9837227463722229, G Loss: 1.3211182355880737\n",
      "Epoch [96/500], Step [300/390], D Loss: 0.9837884902954102, G Loss: 1.3551371097564697\n",
      "Starting epoch 97/500\n",
      "Epoch [97/500], Step [100/390], D Loss: 1.0740852355957031, G Loss: 1.407287836074829\n",
      "Epoch [97/500], Step [200/390], D Loss: 1.1794456243515015, G Loss: 1.3821008205413818\n",
      "Epoch [97/500], Step [300/390], D Loss: 0.9102463722229004, G Loss: 1.4582892656326294\n",
      "Starting epoch 98/500\n",
      "Epoch [98/500], Step [100/390], D Loss: 1.0394182205200195, G Loss: 1.271193504333496\n",
      "Epoch [98/500], Step [200/390], D Loss: 0.9877194166183472, G Loss: 1.3611770868301392\n",
      "Epoch [98/500], Step [300/390], D Loss: 1.1481170654296875, G Loss: 1.2898139953613281\n",
      "Starting epoch 99/500\n",
      "Epoch [99/500], Step [100/390], D Loss: 1.0119972229003906, G Loss: 1.4981454610824585\n",
      "Epoch [99/500], Step [200/390], D Loss: 1.1479971408843994, G Loss: 1.2619307041168213\n",
      "Epoch [99/500], Step [300/390], D Loss: 1.10972261428833, G Loss: 1.2530736923217773\n",
      "Starting epoch 100/500\n",
      "Epoch [100/500], Step [100/390], D Loss: 1.072882890701294, G Loss: 1.3303115367889404\n",
      "Epoch [100/500], Step [200/390], D Loss: 1.0624977350234985, G Loss: 1.2060593366622925\n",
      "Epoch [100/500], Step [300/390], D Loss: 0.854392409324646, G Loss: 1.34586501121521\n",
      "Starting epoch 101/500\n",
      "Epoch [101/500], Step [100/390], D Loss: 0.967778742313385, G Loss: 1.248358964920044\n",
      "Epoch [101/500], Step [200/390], D Loss: 0.9718344211578369, G Loss: 1.4196958541870117\n",
      "Epoch [101/500], Step [300/390], D Loss: 0.9910496473312378, G Loss: 1.276552438735962\n",
      "Starting epoch 102/500\n",
      "Epoch [102/500], Step [100/390], D Loss: 0.984377384185791, G Loss: 1.3106415271759033\n",
      "Epoch [102/500], Step [200/390], D Loss: 1.0219414234161377, G Loss: 1.1275992393493652\n",
      "Epoch [102/500], Step [300/390], D Loss: 1.1117470264434814, G Loss: 1.3798630237579346\n",
      "Starting epoch 103/500\n",
      "Epoch [103/500], Step [100/390], D Loss: 0.971036970615387, G Loss: 1.3046519756317139\n",
      "Epoch [103/500], Step [200/390], D Loss: 0.9357835054397583, G Loss: 1.414729356765747\n",
      "Epoch [103/500], Step [300/390], D Loss: 0.9886417388916016, G Loss: 1.166717529296875\n",
      "Starting epoch 104/500\n",
      "Epoch [104/500], Step [100/390], D Loss: 0.8469201922416687, G Loss: 1.5069113969802856\n",
      "Epoch [104/500], Step [200/390], D Loss: 0.9085787534713745, G Loss: 1.4496381282806396\n",
      "Epoch [104/500], Step [300/390], D Loss: 0.977789044380188, G Loss: 1.261688232421875\n",
      "Starting epoch 105/500\n",
      "Epoch [105/500], Step [100/390], D Loss: 0.9670724868774414, G Loss: 1.4409480094909668\n",
      "Epoch [105/500], Step [200/390], D Loss: 1.0208570957183838, G Loss: 1.3891092538833618\n",
      "Epoch [105/500], Step [300/390], D Loss: 1.0261259078979492, G Loss: 1.2127339839935303\n",
      "Starting epoch 106/500\n",
      "Epoch [106/500], Step [100/390], D Loss: 0.9771353602409363, G Loss: 1.3222191333770752\n",
      "Epoch [106/500], Step [200/390], D Loss: 0.9642117023468018, G Loss: 1.7168174982070923\n",
      "Epoch [106/500], Step [300/390], D Loss: 1.061212182044983, G Loss: 1.2744414806365967\n",
      "Starting epoch 107/500\n",
      "Epoch [107/500], Step [100/390], D Loss: 1.0090234279632568, G Loss: 1.3519835472106934\n",
      "Epoch [107/500], Step [200/390], D Loss: 1.0306987762451172, G Loss: 1.2552520036697388\n",
      "Epoch [107/500], Step [300/390], D Loss: 1.033778429031372, G Loss: 1.2079174518585205\n",
      "Starting epoch 108/500\n",
      "Epoch [108/500], Step [100/390], D Loss: 1.0789899826049805, G Loss: 1.284278392791748\n",
      "Epoch [108/500], Step [200/390], D Loss: 1.0146979093551636, G Loss: 1.3618358373641968\n",
      "Epoch [108/500], Step [300/390], D Loss: 0.8811155557632446, G Loss: 1.3277344703674316\n",
      "Starting epoch 109/500\n",
      "Epoch [109/500], Step [100/390], D Loss: 0.9015516638755798, G Loss: 1.4002574682235718\n",
      "Epoch [109/500], Step [200/390], D Loss: 1.039124608039856, G Loss: 1.3767119646072388\n",
      "Epoch [109/500], Step [300/390], D Loss: 1.1253697872161865, G Loss: 1.17409086227417\n",
      "Starting epoch 110/500\n",
      "Epoch [110/500], Step [100/390], D Loss: 1.0603597164154053, G Loss: 1.2824089527130127\n",
      "Epoch [110/500], Step [200/390], D Loss: 1.038293719291687, G Loss: 1.3141047954559326\n",
      "Epoch [110/500], Step [300/390], D Loss: 1.0183944702148438, G Loss: 1.318231225013733\n",
      "Starting epoch 111/500\n",
      "Epoch [111/500], Step [100/390], D Loss: 1.1137559413909912, G Loss: 1.3792990446090698\n",
      "Epoch [111/500], Step [200/390], D Loss: 0.9802567958831787, G Loss: 1.2836147546768188\n",
      "Epoch [111/500], Step [300/390], D Loss: 0.9301040172576904, G Loss: 1.404760479927063\n",
      "Starting epoch 112/500\n",
      "Epoch [112/500], Step [100/390], D Loss: 0.991709291934967, G Loss: 1.4630882740020752\n",
      "Epoch [112/500], Step [200/390], D Loss: 0.9773650169372559, G Loss: 1.297241449356079\n",
      "Epoch [112/500], Step [300/390], D Loss: 1.17366623878479, G Loss: 1.436997413635254\n",
      "Starting epoch 113/500\n",
      "Epoch [113/500], Step [100/390], D Loss: 0.9886267185211182, G Loss: 1.291583776473999\n",
      "Epoch [113/500], Step [200/390], D Loss: 1.053534746170044, G Loss: 1.2535121440887451\n",
      "Epoch [113/500], Step [300/390], D Loss: 0.9282304048538208, G Loss: 1.4004184007644653\n",
      "Starting epoch 114/500\n",
      "Epoch [114/500], Step [100/390], D Loss: 0.9469072818756104, G Loss: 1.1842801570892334\n",
      "Epoch [114/500], Step [200/390], D Loss: 1.0450215339660645, G Loss: 1.2549800872802734\n",
      "Epoch [114/500], Step [300/390], D Loss: 1.0195653438568115, G Loss: 1.3275818824768066\n",
      "Starting epoch 115/500\n",
      "Epoch [115/500], Step [100/390], D Loss: 0.8680657148361206, G Loss: 1.283184289932251\n",
      "Epoch [115/500], Step [200/390], D Loss: 0.8860223889350891, G Loss: 1.4866384267807007\n",
      "Epoch [115/500], Step [300/390], D Loss: 0.9380325078964233, G Loss: 1.4537665843963623\n",
      "Starting epoch 116/500\n",
      "Epoch [116/500], Step [100/390], D Loss: 1.0492230653762817, G Loss: 1.2903589010238647\n",
      "Epoch [116/500], Step [200/390], D Loss: 0.994223952293396, G Loss: 1.3037540912628174\n",
      "Epoch [116/500], Step [300/390], D Loss: 0.9916027188301086, G Loss: 1.4344618320465088\n",
      "Starting epoch 117/500\n",
      "Epoch [117/500], Step [100/390], D Loss: 0.9131115674972534, G Loss: 1.3709989786148071\n",
      "Epoch [117/500], Step [200/390], D Loss: 1.1202484369277954, G Loss: 1.3367533683776855\n",
      "Epoch [117/500], Step [300/390], D Loss: 0.9995241761207581, G Loss: 1.3272619247436523\n",
      "Starting epoch 118/500\n",
      "Epoch [118/500], Step [100/390], D Loss: 0.9435774087905884, G Loss: 1.4474799633026123\n",
      "Epoch [118/500], Step [200/390], D Loss: 1.035373568534851, G Loss: 1.3020201921463013\n",
      "Epoch [118/500], Step [300/390], D Loss: 0.9400287866592407, G Loss: 1.5828049182891846\n",
      "Starting epoch 119/500\n",
      "Epoch [119/500], Step [100/390], D Loss: 0.9568595886230469, G Loss: 1.30476713180542\n",
      "Epoch [119/500], Step [200/390], D Loss: 1.1397755146026611, G Loss: 1.3955109119415283\n",
      "Epoch [119/500], Step [300/390], D Loss: 1.0343244075775146, G Loss: 1.3509827852249146\n",
      "Starting epoch 120/500\n",
      "Epoch [120/500], Step [100/390], D Loss: 1.0633127689361572, G Loss: 1.4582054615020752\n",
      "Epoch [120/500], Step [200/390], D Loss: 0.9495654106140137, G Loss: 1.2636833190917969\n",
      "Epoch [120/500], Step [300/390], D Loss: 0.9968686103820801, G Loss: 1.2888034582138062\n",
      "Starting epoch 121/500\n",
      "Epoch [121/500], Step [100/390], D Loss: 1.0114977359771729, G Loss: 1.2484718561172485\n",
      "Epoch [121/500], Step [200/390], D Loss: 0.9214120507240295, G Loss: 1.4831817150115967\n",
      "Epoch [121/500], Step [300/390], D Loss: 1.098235845565796, G Loss: 1.449901819229126\n",
      "Starting epoch 122/500\n",
      "Epoch [122/500], Step [100/390], D Loss: 1.0423243045806885, G Loss: 1.3368732929229736\n",
      "Epoch [122/500], Step [200/390], D Loss: 0.9764237999916077, G Loss: 1.303938627243042\n",
      "Epoch [122/500], Step [300/390], D Loss: 1.0540473461151123, G Loss: 1.3511073589324951\n",
      "Starting epoch 123/500\n",
      "Epoch [123/500], Step [100/390], D Loss: 1.0429365634918213, G Loss: 1.2533135414123535\n",
      "Epoch [123/500], Step [200/390], D Loss: 1.021162748336792, G Loss: 1.2919657230377197\n",
      "Epoch [123/500], Step [300/390], D Loss: 0.9454742670059204, G Loss: 1.3531911373138428\n",
      "Starting epoch 124/500\n",
      "Epoch [124/500], Step [100/390], D Loss: 0.89061439037323, G Loss: 1.465040683746338\n",
      "Epoch [124/500], Step [200/390], D Loss: 1.11990487575531, G Loss: 1.3432042598724365\n",
      "Epoch [124/500], Step [300/390], D Loss: 1.1089800596237183, G Loss: 1.236147165298462\n",
      "Starting epoch 125/500\n",
      "Epoch [125/500], Step [100/390], D Loss: 1.0464543104171753, G Loss: 1.2633650302886963\n",
      "Epoch [125/500], Step [200/390], D Loss: 1.0345661640167236, G Loss: 1.3620938062667847\n",
      "Epoch [125/500], Step [300/390], D Loss: 0.9898726940155029, G Loss: 1.133742332458496\n",
      "Starting epoch 126/500\n",
      "Epoch [126/500], Step [100/390], D Loss: 0.9837822914123535, G Loss: 1.2797954082489014\n",
      "Epoch [126/500], Step [200/390], D Loss: 1.0199427604675293, G Loss: 1.211006999015808\n",
      "Epoch [126/500], Step [300/390], D Loss: 1.0399880409240723, G Loss: 1.4709692001342773\n",
      "Starting epoch 127/500\n",
      "Epoch [127/500], Step [100/390], D Loss: 1.0007390975952148, G Loss: 1.2691099643707275\n",
      "Epoch [127/500], Step [200/390], D Loss: 0.9569174647331238, G Loss: 1.304389476776123\n",
      "Epoch [127/500], Step [300/390], D Loss: 1.0702593326568604, G Loss: 1.2213857173919678\n",
      "Starting epoch 128/500\n",
      "Epoch [128/500], Step [100/390], D Loss: 1.0578597784042358, G Loss: 1.3087153434753418\n",
      "Epoch [128/500], Step [200/390], D Loss: 1.113976240158081, G Loss: 1.1891059875488281\n",
      "Epoch [128/500], Step [300/390], D Loss: 1.0594804286956787, G Loss: 1.3282390832901\n",
      "Starting epoch 129/500\n",
      "Epoch [129/500], Step [100/390], D Loss: 1.0431175231933594, G Loss: 1.1451747417449951\n",
      "Epoch [129/500], Step [200/390], D Loss: 1.1379876136779785, G Loss: 1.233717441558838\n",
      "Epoch [129/500], Step [300/390], D Loss: 1.047930359840393, G Loss: 1.2330303192138672\n",
      "Starting epoch 130/500\n",
      "Epoch [130/500], Step [100/390], D Loss: 1.0628764629364014, G Loss: 1.249464750289917\n",
      "Epoch [130/500], Step [200/390], D Loss: 1.07828688621521, G Loss: 1.3639585971832275\n",
      "Epoch [130/500], Step [300/390], D Loss: 1.1339448690414429, G Loss: 1.196728229522705\n",
      "Starting epoch 131/500\n",
      "Epoch [131/500], Step [100/390], D Loss: 1.0698186159133911, G Loss: 1.2730646133422852\n",
      "Epoch [131/500], Step [200/390], D Loss: 0.9731218218803406, G Loss: 1.3144450187683105\n",
      "Epoch [131/500], Step [300/390], D Loss: 0.9414117932319641, G Loss: 1.1674672365188599\n",
      "Starting epoch 132/500\n",
      "Epoch [132/500], Step [100/390], D Loss: 1.0514873266220093, G Loss: 1.3636226654052734\n",
      "Epoch [132/500], Step [200/390], D Loss: 0.9749833345413208, G Loss: 1.351059079170227\n",
      "Epoch [132/500], Step [300/390], D Loss: 1.1300146579742432, G Loss: 1.373531699180603\n",
      "Starting epoch 133/500\n",
      "Epoch [133/500], Step [100/390], D Loss: 0.9042078256607056, G Loss: 1.3774449825286865\n",
      "Epoch [133/500], Step [200/390], D Loss: 1.0155458450317383, G Loss: 1.218958854675293\n",
      "Epoch [133/500], Step [300/390], D Loss: 1.0763764381408691, G Loss: 1.2428244352340698\n",
      "Starting epoch 134/500\n",
      "Epoch [134/500], Step [100/390], D Loss: 1.0112903118133545, G Loss: 1.347813367843628\n",
      "Epoch [134/500], Step [200/390], D Loss: 0.9999223947525024, G Loss: 1.3763563632965088\n",
      "Epoch [134/500], Step [300/390], D Loss: 0.9912892580032349, G Loss: 1.2612719535827637\n",
      "Starting epoch 135/500\n",
      "Epoch [135/500], Step [100/390], D Loss: 1.0393694639205933, G Loss: 1.2381291389465332\n",
      "Epoch [135/500], Step [200/390], D Loss: 0.9864721298217773, G Loss: 1.3186224699020386\n",
      "Epoch [135/500], Step [300/390], D Loss: 1.043823480606079, G Loss: 1.2135920524597168\n",
      "Starting epoch 136/500\n",
      "Epoch [136/500], Step [100/390], D Loss: 0.9843609929084778, G Loss: 1.3795146942138672\n",
      "Epoch [136/500], Step [200/390], D Loss: 1.0458108186721802, G Loss: 1.3359568119049072\n",
      "Epoch [136/500], Step [300/390], D Loss: 1.0246682167053223, G Loss: 1.2831693887710571\n",
      "Starting epoch 137/500\n",
      "Epoch [137/500], Step [100/390], D Loss: 1.0710281133651733, G Loss: 1.249313473701477\n",
      "Epoch [137/500], Step [200/390], D Loss: 1.0512440204620361, G Loss: 1.3599612712860107\n",
      "Epoch [137/500], Step [300/390], D Loss: 0.9928712844848633, G Loss: 1.2550033330917358\n",
      "Starting epoch 138/500\n",
      "Epoch [138/500], Step [100/390], D Loss: 1.0041923522949219, G Loss: 1.4756758213043213\n",
      "Epoch [138/500], Step [200/390], D Loss: 0.9282853603363037, G Loss: 1.6338258981704712\n",
      "Epoch [138/500], Step [300/390], D Loss: 1.298833966255188, G Loss: 1.343430519104004\n",
      "Starting epoch 139/500\n",
      "Epoch [139/500], Step [100/390], D Loss: 0.9908551573753357, G Loss: 1.1788523197174072\n",
      "Epoch [139/500], Step [200/390], D Loss: 0.9678066372871399, G Loss: 1.2564131021499634\n",
      "Epoch [139/500], Step [300/390], D Loss: 1.0473825931549072, G Loss: 1.3173243999481201\n",
      "Starting epoch 140/500\n",
      "Epoch [140/500], Step [100/390], D Loss: 0.9423723816871643, G Loss: 1.1860313415527344\n",
      "Epoch [140/500], Step [200/390], D Loss: 1.0474578142166138, G Loss: 1.2111752033233643\n",
      "Epoch [140/500], Step [300/390], D Loss: 0.9140821695327759, G Loss: 1.4232773780822754\n",
      "Starting epoch 141/500\n",
      "Epoch [141/500], Step [100/390], D Loss: 0.9768428206443787, G Loss: 1.2739224433898926\n",
      "Epoch [141/500], Step [200/390], D Loss: 0.9931062459945679, G Loss: 1.1755352020263672\n",
      "Epoch [141/500], Step [300/390], D Loss: 1.131636619567871, G Loss: 1.190764307975769\n",
      "Starting epoch 142/500\n",
      "Epoch [142/500], Step [100/390], D Loss: 1.0004169940948486, G Loss: 1.1553711891174316\n",
      "Epoch [142/500], Step [200/390], D Loss: 1.0231456756591797, G Loss: 1.424001932144165\n",
      "Epoch [142/500], Step [300/390], D Loss: 1.0429338216781616, G Loss: 1.3880319595336914\n",
      "Starting epoch 143/500\n",
      "Epoch [143/500], Step [100/390], D Loss: 0.9539509415626526, G Loss: 1.2889429330825806\n",
      "Epoch [143/500], Step [200/390], D Loss: 0.9527244567871094, G Loss: 1.3906205892562866\n",
      "Epoch [143/500], Step [300/390], D Loss: 1.0107556581497192, G Loss: 1.105560541152954\n",
      "Starting epoch 144/500\n",
      "Epoch [144/500], Step [100/390], D Loss: 0.9596685171127319, G Loss: 1.3302252292633057\n",
      "Epoch [144/500], Step [200/390], D Loss: 1.0791735649108887, G Loss: 1.195868730545044\n",
      "Epoch [144/500], Step [300/390], D Loss: 0.9986554384231567, G Loss: 1.4638209342956543\n",
      "Starting epoch 145/500\n",
      "Epoch [145/500], Step [100/390], D Loss: 1.0335901975631714, G Loss: 1.2749519348144531\n",
      "Epoch [145/500], Step [200/390], D Loss: 1.0461962223052979, G Loss: 1.2868165969848633\n",
      "Epoch [145/500], Step [300/390], D Loss: 1.1124050617218018, G Loss: 1.2586784362792969\n",
      "Starting epoch 146/500\n",
      "Epoch [146/500], Step [100/390], D Loss: 1.060227394104004, G Loss: 1.3047481775283813\n",
      "Epoch [146/500], Step [200/390], D Loss: 1.0534288883209229, G Loss: 1.24931001663208\n",
      "Epoch [146/500], Step [300/390], D Loss: 0.894045352935791, G Loss: 1.3897922039031982\n",
      "Starting epoch 147/500\n",
      "Epoch [147/500], Step [100/390], D Loss: 0.9856308102607727, G Loss: 1.232431173324585\n",
      "Epoch [147/500], Step [200/390], D Loss: 1.030927300453186, G Loss: 1.4326517581939697\n",
      "Epoch [147/500], Step [300/390], D Loss: 1.0011098384857178, G Loss: 1.3475046157836914\n",
      "Starting epoch 148/500\n",
      "Epoch [148/500], Step [100/390], D Loss: 0.8773164749145508, G Loss: 1.4522364139556885\n",
      "Epoch [148/500], Step [200/390], D Loss: 0.9509104490280151, G Loss: 1.4883379936218262\n",
      "Epoch [148/500], Step [300/390], D Loss: 1.0592103004455566, G Loss: 1.26370108127594\n",
      "Starting epoch 149/500\n",
      "Epoch [149/500], Step [100/390], D Loss: 1.0197514295578003, G Loss: 1.1695311069488525\n",
      "Epoch [149/500], Step [200/390], D Loss: 1.0860238075256348, G Loss: 1.3317935466766357\n",
      "Epoch [149/500], Step [300/390], D Loss: 1.086883783340454, G Loss: 1.279792308807373\n",
      "Starting epoch 150/500\n",
      "Epoch [150/500], Step [100/390], D Loss: 1.0123794078826904, G Loss: 1.2775168418884277\n",
      "Epoch [150/500], Step [200/390], D Loss: 1.0697726011276245, G Loss: 1.351649522781372\n",
      "Epoch [150/500], Step [300/390], D Loss: 1.0377318859100342, G Loss: 1.3130306005477905\n",
      "Starting epoch 151/500\n",
      "Epoch [151/500], Step [100/390], D Loss: 1.0115081071853638, G Loss: 1.1811678409576416\n",
      "Epoch [151/500], Step [200/390], D Loss: 0.9788578748703003, G Loss: 1.3766876459121704\n",
      "Epoch [151/500], Step [300/390], D Loss: 0.9291950464248657, G Loss: 1.3026230335235596\n",
      "Starting epoch 152/500\n",
      "Epoch [152/500], Step [100/390], D Loss: 0.9587106704711914, G Loss: 1.3575763702392578\n",
      "Epoch [152/500], Step [200/390], D Loss: 0.9268912076950073, G Loss: 1.245046615600586\n",
      "Epoch [152/500], Step [300/390], D Loss: 0.9949399828910828, G Loss: 1.2643985748291016\n",
      "Starting epoch 153/500\n",
      "Epoch [153/500], Step [100/390], D Loss: 0.952817976474762, G Loss: 1.2409274578094482\n",
      "Epoch [153/500], Step [200/390], D Loss: 1.0705374479293823, G Loss: 1.3161503076553345\n",
      "Epoch [153/500], Step [300/390], D Loss: 0.9437157511711121, G Loss: 1.2887072563171387\n",
      "Starting epoch 154/500\n",
      "Epoch [154/500], Step [100/390], D Loss: 1.0469896793365479, G Loss: 1.3102383613586426\n",
      "Epoch [154/500], Step [200/390], D Loss: 1.02189040184021, G Loss: 1.2184998989105225\n",
      "Epoch [154/500], Step [300/390], D Loss: 0.9398951530456543, G Loss: 1.2287042140960693\n",
      "Starting epoch 155/500\n",
      "Epoch [155/500], Step [100/390], D Loss: 1.0815937519073486, G Loss: 1.2666093111038208\n",
      "Epoch [155/500], Step [200/390], D Loss: 1.0332565307617188, G Loss: 1.3230626583099365\n",
      "Epoch [155/500], Step [300/390], D Loss: 1.1082473993301392, G Loss: 1.2509645223617554\n",
      "Starting epoch 156/500\n",
      "Epoch [156/500], Step [100/390], D Loss: 0.9890844821929932, G Loss: 1.4477839469909668\n",
      "Epoch [156/500], Step [200/390], D Loss: 0.8630008697509766, G Loss: 1.2783493995666504\n",
      "Epoch [156/500], Step [300/390], D Loss: 1.0445586442947388, G Loss: 1.2431081533432007\n",
      "Starting epoch 157/500\n",
      "Epoch [157/500], Step [100/390], D Loss: 1.108583688735962, G Loss: 1.231384515762329\n",
      "Epoch [157/500], Step [200/390], D Loss: 0.9964825510978699, G Loss: 1.35697340965271\n",
      "Epoch [157/500], Step [300/390], D Loss: 0.9960606694221497, G Loss: 1.2870932817459106\n",
      "Starting epoch 158/500\n",
      "Epoch [158/500], Step [100/390], D Loss: 0.9908478260040283, G Loss: 1.3472074270248413\n",
      "Epoch [158/500], Step [200/390], D Loss: 0.9957716464996338, G Loss: 1.2517353296279907\n"
     ]
    }
   ],
   "source": [
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Optimizers\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting epoch {epoch+1}/{num_epochs}')\n",
    "    for i, (images, _) in enumerate(train_loader_cifar10):\n",
    "        real_images = images.to(device)\n",
    "        batch_size = real_images.size(0)\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        # --------- Train the Discriminator --------- #\n",
    "        d_optimizer.zero_grad()\n",
    "        outputs = discriminator(real_images)\n",
    "        d_real_loss = criterion(outputs, real_labels)\n",
    "        z = torch.randn(batch_size, latent_dim).to(device)\n",
    "        fake_images = generator(z)\n",
    "        outputs = discriminator(fake_images.detach())\n",
    "        d_fake_loss = criterion(outputs, fake_labels)\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # --------- Train the Generator --------- #\n",
    "        g_optimizer.zero_grad()\n",
    "        outputs = discriminator(fake_images)\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:  # More frequent logging\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader_cifar10)}], D Loss: {d_loss.item()}, G Loss: {g_loss.item()}')\n",
    "\n",
    "    # Save generated images every epoch\n",
    "    save_image(fake_images.data[:25], f'./data/cifar10/fake_image_{epoch+1:03d}.png', nrow=5, normalize=True)\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
