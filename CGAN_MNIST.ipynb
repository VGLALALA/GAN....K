{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:29.025651526Z",
     "start_time": "2024-05-20T02:17:29.017840935Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "lr = 0.0002\n",
    "num_classes = 10\n",
    "batch_size = 128\n",
    "img_size = 28\n",
    "num_epochs = 100\n",
    "n_images = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:29.061376006Z",
     "start_time": "2024-05-20T02:17:29.020058911Z"
    }
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda')\n",
    "os.makedirs('./data/mnist', exist_ok=True)\n",
    "# MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:29.061574752Z",
     "start_time": "2024-05-20T02:17:29.061006178Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_channels=1):\n",
    "        super(Generator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "\n",
    "        self.init_size = img_size // 4  # Initial size before upsampling\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim + num_classes, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(64, img_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        out = self.l1(gen_input)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.model(out)\n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.label_embedding = nn.Embedding(num_classes, img_size * img_size)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(img_channels + 1, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),    \n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.adv_layer = nn.Sequential(\n",
    "            nn.Linear(512 * 2 * 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        label_embeddings = self.label_embedding(labels).view(labels.size(0), 1, img_size, img_size)\n",
    "        d_in = torch.cat((img, label_embeddings), 1)\n",
    "        out = self.model(d_in)\n",
    "        out = out.view(out.size(0), -1) \n",
    "        validity = self.adv_layer(out)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def generate_images(generator, epoch, n_images, latent_dim, digit=6):\n",
    "    z = torch.randn(n_images, latent_dim, device=device)\n",
    "    labels = torch.full((n_images,), digit, dtype=torch.long, device=device)\n",
    "    gen_imgs = generator(z, labels)\n",
    "    gen_imgs = gen_imgs * 0.5 + 0.5\n",
    "    save_image(gen_imgs.data, f'./data/mnist/cgan/fake_image_{epoch+1:03d}_digit_{digit}.png', nrow=5, normalize=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:29.061680591Z",
     "start_time": "2024-05-20T02:17:29.061146794Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:22:41.250098465Z",
     "start_time": "2024-05-20T02:17:29.061195736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 0/468] [D loss: 0.6942203044891357] [G loss: 0.6911160945892334]\n",
      "[Epoch 0/100] [Batch 100/468] [D loss: 0.5465178489685059] [G loss: 1.1285897493362427]\n",
      "[Epoch 0/100] [Batch 200/468] [D loss: 0.690596878528595] [G loss: 0.6928708553314209]\n",
      "[Epoch 0/100] [Batch 300/468] [D loss: 0.6494032144546509] [G loss: 0.8030210733413696]\n",
      "[Epoch 0/100] [Batch 400/468] [D loss: 0.6812285780906677] [G loss: 0.7501981258392334]\n",
      "[Epoch 1/100] [Batch 0/468] [D loss: 0.6362183690071106] [G loss: 0.8183027505874634]\n",
      "[Epoch 1/100] [Batch 100/468] [D loss: 0.6693100929260254] [G loss: 0.8404824733734131]\n",
      "[Epoch 1/100] [Batch 200/468] [D loss: 0.6495645046234131] [G loss: 0.8573331832885742]\n",
      "[Epoch 1/100] [Batch 300/468] [D loss: 0.5559887886047363] [G loss: 0.9519342184066772]\n",
      "[Epoch 1/100] [Batch 400/468] [D loss: 0.5796309113502502] [G loss: 0.9852864742279053]\n",
      "[Epoch 2/100] [Batch 0/468] [D loss: 0.540535569190979] [G loss: 0.9175381660461426]\n",
      "[Epoch 2/100] [Batch 100/468] [D loss: 0.5420811176300049] [G loss: 1.128137230873108]\n",
      "[Epoch 2/100] [Batch 200/468] [D loss: 0.5634508728981018] [G loss: 1.142986536026001]\n",
      "[Epoch 2/100] [Batch 300/468] [D loss: 0.580176591873169] [G loss: 1.050767183303833]\n",
      "[Epoch 2/100] [Batch 400/468] [D loss: 0.5874406695365906] [G loss: 0.8077058792114258]\n",
      "[Epoch 3/100] [Batch 0/468] [D loss: 0.6273832321166992] [G loss: 0.8902571201324463]\n",
      "[Epoch 3/100] [Batch 100/468] [D loss: 0.6214669942855835] [G loss: 0.9163416624069214]\n",
      "[Epoch 3/100] [Batch 200/468] [D loss: 0.613559901714325] [G loss: 0.8870359063148499]\n",
      "[Epoch 3/100] [Batch 300/468] [D loss: 0.6377473473548889] [G loss: 0.8220314979553223]\n",
      "[Epoch 3/100] [Batch 400/468] [D loss: 0.6169191002845764] [G loss: 0.8839449882507324]\n",
      "[Epoch 4/100] [Batch 0/468] [D loss: 0.6493532657623291] [G loss: 0.8322864174842834]\n",
      "[Epoch 4/100] [Batch 100/468] [D loss: 0.6481293439865112] [G loss: 0.8992367386817932]\n",
      "[Epoch 4/100] [Batch 200/468] [D loss: 0.6354029178619385] [G loss: 0.8101634979248047]\n",
      "[Epoch 4/100] [Batch 300/468] [D loss: 0.6603589057922363] [G loss: 0.7803382277488708]\n",
      "[Epoch 4/100] [Batch 400/468] [D loss: 0.6886866092681885] [G loss: 0.7556759119033813]\n",
      "[Epoch 5/100] [Batch 0/468] [D loss: 0.6213138699531555] [G loss: 0.9274553656578064]\n",
      "[Epoch 5/100] [Batch 100/468] [D loss: 0.6221212148666382] [G loss: 0.7711933851242065]\n",
      "[Epoch 5/100] [Batch 200/468] [D loss: 0.6372697949409485] [G loss: 0.8347919583320618]\n",
      "[Epoch 5/100] [Batch 300/468] [D loss: 0.6622339487075806] [G loss: 0.9232099056243896]\n",
      "[Epoch 5/100] [Batch 400/468] [D loss: 0.6448124647140503] [G loss: 0.8255865573883057]\n",
      "[Epoch 6/100] [Batch 0/468] [D loss: 0.6739113926887512] [G loss: 0.8317074775695801]\n",
      "[Epoch 6/100] [Batch 100/468] [D loss: 0.6334362030029297] [G loss: 0.9353744983673096]\n",
      "[Epoch 6/100] [Batch 200/468] [D loss: 0.6391909122467041] [G loss: 0.8319891691207886]\n",
      "[Epoch 6/100] [Batch 300/468] [D loss: 0.6536747217178345] [G loss: 0.8868029117584229]\n",
      "[Epoch 6/100] [Batch 400/468] [D loss: 0.6502640843391418] [G loss: 0.6972130537033081]\n",
      "[Epoch 7/100] [Batch 0/468] [D loss: 0.6599936485290527] [G loss: 0.7699136137962341]\n",
      "[Epoch 7/100] [Batch 100/468] [D loss: 0.631252110004425] [G loss: 0.6776970624923706]\n",
      "[Epoch 7/100] [Batch 200/468] [D loss: 0.6789178848266602] [G loss: 0.7975320816040039]\n",
      "[Epoch 7/100] [Batch 300/468] [D loss: 0.6554792523384094] [G loss: 0.7845921516418457]\n",
      "[Epoch 7/100] [Batch 400/468] [D loss: 0.6613540053367615] [G loss: 0.7482292056083679]\n",
      "[Epoch 8/100] [Batch 0/468] [D loss: 0.6845588684082031] [G loss: 0.7848578691482544]\n",
      "[Epoch 8/100] [Batch 100/468] [D loss: 0.7394405007362366] [G loss: 0.7509177923202515]\n",
      "[Epoch 8/100] [Batch 200/468] [D loss: 0.6751192808151245] [G loss: 0.8440623879432678]\n",
      "[Epoch 8/100] [Batch 300/468] [D loss: 0.711132287979126] [G loss: 0.7120934724807739]\n",
      "[Epoch 8/100] [Batch 400/468] [D loss: 0.6543385982513428] [G loss: 0.797286331653595]\n",
      "[Epoch 9/100] [Batch 0/468] [D loss: 0.6668411493301392] [G loss: 0.8062409162521362]\n",
      "[Epoch 9/100] [Batch 100/468] [D loss: 0.6622505784034729] [G loss: 0.6765390634536743]\n",
      "[Epoch 9/100] [Batch 200/468] [D loss: 0.6554741859436035] [G loss: 0.8368524312973022]\n",
      "[Epoch 9/100] [Batch 300/468] [D loss: 0.6402973532676697] [G loss: 0.7282019853591919]\n",
      "[Epoch 9/100] [Batch 400/468] [D loss: 0.6788498163223267] [G loss: 0.6932684779167175]\n",
      "[Epoch 10/100] [Batch 0/468] [D loss: 0.6400859355926514] [G loss: 0.6673388481140137]\n",
      "[Epoch 10/100] [Batch 100/468] [D loss: 0.6656883955001831] [G loss: 0.6973188519477844]\n",
      "[Epoch 10/100] [Batch 200/468] [D loss: 0.6715313196182251] [G loss: 0.6606897711753845]\n",
      "[Epoch 10/100] [Batch 300/468] [D loss: 0.6451573371887207] [G loss: 0.9889338612556458]\n",
      "[Epoch 10/100] [Batch 400/468] [D loss: 0.6768620014190674] [G loss: 0.9700340628623962]\n",
      "[Epoch 11/100] [Batch 0/468] [D loss: 0.6692464351654053] [G loss: 0.7222674489021301]\n",
      "[Epoch 11/100] [Batch 100/468] [D loss: 0.7222002744674683] [G loss: 0.779218316078186]\n",
      "[Epoch 11/100] [Batch 200/468] [D loss: 0.6817746162414551] [G loss: 0.8479384183883667]\n",
      "[Epoch 11/100] [Batch 300/468] [D loss: 0.6814956068992615] [G loss: 0.8630608320236206]\n",
      "[Epoch 11/100] [Batch 400/468] [D loss: 0.6665306091308594] [G loss: 0.6349166631698608]\n",
      "[Epoch 12/100] [Batch 0/468] [D loss: 0.6664429903030396] [G loss: 0.6505109071731567]\n",
      "[Epoch 12/100] [Batch 100/468] [D loss: 0.666325569152832] [G loss: 0.8589109778404236]\n",
      "[Epoch 12/100] [Batch 200/468] [D loss: 0.6864783763885498] [G loss: 0.8068958520889282]\n",
      "[Epoch 12/100] [Batch 300/468] [D loss: 0.6635405421257019] [G loss: 0.6594464778900146]\n",
      "[Epoch 12/100] [Batch 400/468] [D loss: 0.6382004022598267] [G loss: 0.657211422920227]\n",
      "[Epoch 13/100] [Batch 0/468] [D loss: 0.6804330348968506] [G loss: 0.6330571174621582]\n",
      "[Epoch 13/100] [Batch 100/468] [D loss: 0.6568914651870728] [G loss: 0.6872650384902954]\n",
      "[Epoch 13/100] [Batch 200/468] [D loss: 0.6953078508377075] [G loss: 0.7460076212882996]\n",
      "[Epoch 13/100] [Batch 300/468] [D loss: 0.6649547219276428] [G loss: 0.6306045055389404]\n",
      "[Epoch 13/100] [Batch 400/468] [D loss: 0.6746678948402405] [G loss: 0.8898848295211792]\n",
      "[Epoch 14/100] [Batch 0/468] [D loss: 0.6507552862167358] [G loss: 0.9642218947410583]\n",
      "[Epoch 14/100] [Batch 100/468] [D loss: 0.6978244781494141] [G loss: 0.6270025968551636]\n",
      "[Epoch 14/100] [Batch 200/468] [D loss: 0.6689649820327759] [G loss: 0.7839188575744629]\n",
      "[Epoch 14/100] [Batch 300/468] [D loss: 0.6519784927368164] [G loss: 1.0233184099197388]\n",
      "[Epoch 14/100] [Batch 400/468] [D loss: 0.6710556149482727] [G loss: 0.7747533917427063]\n",
      "[Epoch 15/100] [Batch 0/468] [D loss: 0.6815472841262817] [G loss: 0.7903196811676025]\n",
      "[Epoch 15/100] [Batch 100/468] [D loss: 0.6760582327842712] [G loss: 0.6825813055038452]\n",
      "[Epoch 15/100] [Batch 200/468] [D loss: 0.6622253656387329] [G loss: 0.8172584176063538]\n",
      "[Epoch 15/100] [Batch 300/468] [D loss: 0.6947854161262512] [G loss: 0.7276136875152588]\n",
      "[Epoch 15/100] [Batch 400/468] [D loss: 0.6753299236297607] [G loss: 0.9681433439254761]\n",
      "[Epoch 16/100] [Batch 0/468] [D loss: 0.6751514673233032] [G loss: 0.8781654238700867]\n",
      "[Epoch 16/100] [Batch 100/468] [D loss: 0.6791670322418213] [G loss: 0.6636683940887451]\n",
      "[Epoch 16/100] [Batch 200/468] [D loss: 0.6507753729820251] [G loss: 0.8102024793624878]\n",
      "[Epoch 16/100] [Batch 300/468] [D loss: 0.6902469992637634] [G loss: 0.7429229021072388]\n",
      "[Epoch 16/100] [Batch 400/468] [D loss: 0.6703027486801147] [G loss: 0.8160384893417358]\n",
      "[Epoch 17/100] [Batch 0/468] [D loss: 0.6779015064239502] [G loss: 0.721781849861145]\n",
      "[Epoch 17/100] [Batch 100/468] [D loss: 0.6937336325645447] [G loss: 0.688121497631073]\n",
      "[Epoch 17/100] [Batch 200/468] [D loss: 0.6794995069503784] [G loss: 0.7382134199142456]\n",
      "[Epoch 17/100] [Batch 300/468] [D loss: 0.6701408624649048] [G loss: 0.6521849632263184]\n",
      "[Epoch 17/100] [Batch 400/468] [D loss: 0.6950000524520874] [G loss: 0.8182092905044556]\n",
      "[Epoch 18/100] [Batch 0/468] [D loss: 0.6767477989196777] [G loss: 0.755603551864624]\n",
      "[Epoch 18/100] [Batch 100/468] [D loss: 0.6780928373336792] [G loss: 0.8173543214797974]\n",
      "[Epoch 18/100] [Batch 200/468] [D loss: 0.6595762968063354] [G loss: 0.727369487285614]\n",
      "[Epoch 18/100] [Batch 300/468] [D loss: 0.6787496209144592] [G loss: 0.7598820924758911]\n",
      "[Epoch 18/100] [Batch 400/468] [D loss: 0.6411558389663696] [G loss: 0.6952781677246094]\n",
      "[Epoch 19/100] [Batch 0/468] [D loss: 0.6644301414489746] [G loss: 0.7630323171615601]\n",
      "[Epoch 19/100] [Batch 100/468] [D loss: 0.698800265789032] [G loss: 0.8068963289260864]\n",
      "[Epoch 19/100] [Batch 200/468] [D loss: 0.686319887638092] [G loss: 0.735957145690918]\n",
      "[Epoch 19/100] [Batch 300/468] [D loss: 0.6837125420570374] [G loss: 0.7572548985481262]\n",
      "[Epoch 19/100] [Batch 400/468] [D loss: 0.7257733345031738] [G loss: 0.6944147944450378]\n",
      "[Epoch 20/100] [Batch 0/468] [D loss: 0.6874802112579346] [G loss: 0.7992132902145386]\n",
      "[Epoch 20/100] [Batch 100/468] [D loss: 0.6646230816841125] [G loss: 0.7092804908752441]\n",
      "[Epoch 20/100] [Batch 200/468] [D loss: 0.6622378826141357] [G loss: 0.7915425300598145]\n",
      "[Epoch 20/100] [Batch 300/468] [D loss: 0.6447898745536804] [G loss: 0.7227990627288818]\n",
      "[Epoch 20/100] [Batch 400/468] [D loss: 0.6482276916503906] [G loss: 0.7174519300460815]\n",
      "[Epoch 21/100] [Batch 0/468] [D loss: 0.6743937730789185] [G loss: 0.6819339990615845]\n",
      "[Epoch 21/100] [Batch 100/468] [D loss: 0.6579830646514893] [G loss: 0.7039409875869751]\n",
      "[Epoch 21/100] [Batch 200/468] [D loss: 0.6598794460296631] [G loss: 0.7368682622909546]\n",
      "[Epoch 21/100] [Batch 300/468] [D loss: 0.6799299716949463] [G loss: 0.7733216285705566]\n",
      "[Epoch 21/100] [Batch 400/468] [D loss: 0.6660501956939697] [G loss: 0.8007978200912476]\n",
      "[Epoch 22/100] [Batch 0/468] [D loss: 0.6666923761367798] [G loss: 0.7942415475845337]\n",
      "[Epoch 22/100] [Batch 100/468] [D loss: 0.6694552898406982] [G loss: 0.6799047589302063]\n",
      "[Epoch 22/100] [Batch 200/468] [D loss: 0.6759697198867798] [G loss: 0.8549147248268127]\n",
      "[Epoch 22/100] [Batch 300/468] [D loss: 0.6908568143844604] [G loss: 0.7301235198974609]\n",
      "[Epoch 22/100] [Batch 400/468] [D loss: 0.6753414273262024] [G loss: 0.7675262689590454]\n",
      "[Epoch 23/100] [Batch 0/468] [D loss: 0.6811624765396118] [G loss: 0.8066797256469727]\n",
      "[Epoch 23/100] [Batch 100/468] [D loss: 0.7053781747817993] [G loss: 0.7270936965942383]\n",
      "[Epoch 23/100] [Batch 200/468] [D loss: 0.6670941710472107] [G loss: 0.8113142251968384]\n",
      "[Epoch 23/100] [Batch 300/468] [D loss: 0.6995728611946106] [G loss: 0.8336191177368164]\n",
      "[Epoch 23/100] [Batch 400/468] [D loss: 0.6485825181007385] [G loss: 0.7795923948287964]\n",
      "[Epoch 24/100] [Batch 0/468] [D loss: 0.6755883693695068] [G loss: 0.779614269733429]\n",
      "[Epoch 24/100] [Batch 100/468] [D loss: 0.6843526363372803] [G loss: 0.7555235624313354]\n",
      "[Epoch 24/100] [Batch 200/468] [D loss: 0.6703317165374756] [G loss: 0.7688459157943726]\n",
      "[Epoch 24/100] [Batch 300/468] [D loss: 0.6784857511520386] [G loss: 0.7125003337860107]\n",
      "[Epoch 24/100] [Batch 400/468] [D loss: 0.6820193529129028] [G loss: 0.7581806182861328]\n",
      "[Epoch 25/100] [Batch 0/468] [D loss: 0.6337414979934692] [G loss: 0.8707900047302246]\n",
      "[Epoch 25/100] [Batch 100/468] [D loss: 0.6648369431495667] [G loss: 0.7306444644927979]\n",
      "[Epoch 25/100] [Batch 200/468] [D loss: 0.6687988042831421] [G loss: 0.8342646360397339]\n",
      "[Epoch 25/100] [Batch 300/468] [D loss: 0.669654130935669] [G loss: 0.8098458647727966]\n",
      "[Epoch 25/100] [Batch 400/468] [D loss: 0.6842899322509766] [G loss: 0.7222660183906555]\n",
      "[Epoch 26/100] [Batch 0/468] [D loss: 0.6496832370758057] [G loss: 0.7613281607627869]\n",
      "[Epoch 26/100] [Batch 100/468] [D loss: 0.6848350763320923] [G loss: 0.7374569773674011]\n",
      "[Epoch 26/100] [Batch 200/468] [D loss: 0.6577242612838745] [G loss: 0.7254307270050049]\n",
      "[Epoch 26/100] [Batch 300/468] [D loss: 0.6777033805847168] [G loss: 0.7468739748001099]\n",
      "[Epoch 26/100] [Batch 400/468] [D loss: 0.6842710971832275] [G loss: 0.6757997870445251]\n",
      "[Epoch 27/100] [Batch 0/468] [D loss: 0.6656015515327454] [G loss: 0.700676679611206]\n",
      "[Epoch 27/100] [Batch 100/468] [D loss: 0.6419263482093811] [G loss: 0.6900293827056885]\n",
      "[Epoch 27/100] [Batch 200/468] [D loss: 0.6348279118537903] [G loss: 0.7121528387069702]\n",
      "[Epoch 27/100] [Batch 300/468] [D loss: 0.7008301615715027] [G loss: 0.7420513033866882]\n",
      "[Epoch 27/100] [Batch 400/468] [D loss: 0.6820948123931885] [G loss: 0.7254583835601807]\n",
      "[Epoch 28/100] [Batch 0/468] [D loss: 0.6486089825630188] [G loss: 0.8044396042823792]\n",
      "[Epoch 28/100] [Batch 100/468] [D loss: 0.6723870635032654] [G loss: 0.769792914390564]\n",
      "[Epoch 28/100] [Batch 200/468] [D loss: 0.6570144891738892] [G loss: 0.8026691675186157]\n",
      "[Epoch 28/100] [Batch 300/468] [D loss: 0.6680431365966797] [G loss: 0.815752387046814]\n",
      "[Epoch 28/100] [Batch 400/468] [D loss: 0.6599260568618774] [G loss: 0.7887827157974243]\n",
      "[Epoch 29/100] [Batch 0/468] [D loss: 0.63791823387146] [G loss: 0.707748532295227]\n",
      "[Epoch 29/100] [Batch 100/468] [D loss: 0.6734182834625244] [G loss: 0.8289279937744141]\n",
      "[Epoch 29/100] [Batch 200/468] [D loss: 0.661607027053833] [G loss: 0.7576858997344971]\n",
      "[Epoch 29/100] [Batch 300/468] [D loss: 0.6366434097290039] [G loss: 0.7439358234405518]\n",
      "[Epoch 29/100] [Batch 400/468] [D loss: 0.6667284369468689] [G loss: 0.7667844295501709]\n",
      "[Epoch 30/100] [Batch 0/468] [D loss: 0.6681308746337891] [G loss: 0.6267998814582825]\n",
      "[Epoch 30/100] [Batch 100/468] [D loss: 0.6699193716049194] [G loss: 0.8004225492477417]\n",
      "[Epoch 30/100] [Batch 200/468] [D loss: 0.6942189931869507] [G loss: 0.8328626155853271]\n",
      "[Epoch 30/100] [Batch 300/468] [D loss: 0.6411744356155396] [G loss: 0.7561319470405579]\n",
      "[Epoch 30/100] [Batch 400/468] [D loss: 0.6791383028030396] [G loss: 0.6963271498680115]\n",
      "[Epoch 31/100] [Batch 0/468] [D loss: 0.6543489694595337] [G loss: 0.7256053686141968]\n",
      "[Epoch 31/100] [Batch 100/468] [D loss: 0.6622872352600098] [G loss: 0.7427830696105957]\n",
      "[Epoch 31/100] [Batch 200/468] [D loss: 0.6770404577255249] [G loss: 0.819866418838501]\n",
      "[Epoch 31/100] [Batch 300/468] [D loss: 0.6787264347076416] [G loss: 0.8199362754821777]\n",
      "[Epoch 31/100] [Batch 400/468] [D loss: 0.6528282165527344] [G loss: 0.7487828731536865]\n",
      "[Epoch 32/100] [Batch 0/468] [D loss: 0.6658809781074524] [G loss: 0.7800494432449341]\n",
      "[Epoch 32/100] [Batch 100/468] [D loss: 0.6960412263870239] [G loss: 0.7810074090957642]\n",
      "[Epoch 32/100] [Batch 200/468] [D loss: 0.6855517625808716] [G loss: 0.8471426963806152]\n",
      "[Epoch 32/100] [Batch 300/468] [D loss: 0.6977871656417847] [G loss: 0.8079094290733337]\n",
      "[Epoch 32/100] [Batch 400/468] [D loss: 0.6480106115341187] [G loss: 0.7596580386161804]\n",
      "[Epoch 33/100] [Batch 0/468] [D loss: 0.6512805223464966] [G loss: 0.7828019261360168]\n",
      "[Epoch 33/100] [Batch 100/468] [D loss: 0.6779717206954956] [G loss: 0.7140535116195679]\n",
      "[Epoch 33/100] [Batch 200/468] [D loss: 0.671578586101532] [G loss: 0.741603672504425]\n",
      "[Epoch 33/100] [Batch 300/468] [D loss: 0.6978700757026672] [G loss: 0.7474250793457031]\n",
      "[Epoch 33/100] [Batch 400/468] [D loss: 0.6610773801803589] [G loss: 0.8172226548194885]\n",
      "[Epoch 34/100] [Batch 0/468] [D loss: 0.6681309938430786] [G loss: 0.7415478825569153]\n",
      "[Epoch 34/100] [Batch 100/468] [D loss: 0.6833382844924927] [G loss: 0.831150233745575]\n",
      "[Epoch 34/100] [Batch 200/468] [D loss: 0.6702640056610107] [G loss: 0.775277853012085]\n",
      "[Epoch 34/100] [Batch 300/468] [D loss: 0.6364420652389526] [G loss: 0.7356679439544678]\n",
      "[Epoch 34/100] [Batch 400/468] [D loss: 0.6559293270111084] [G loss: 0.779655396938324]\n",
      "[Epoch 35/100] [Batch 0/468] [D loss: 0.671042263507843] [G loss: 0.7310047149658203]\n",
      "[Epoch 35/100] [Batch 100/468] [D loss: 0.6720700860023499] [G loss: 0.7094802856445312]\n",
      "[Epoch 35/100] [Batch 200/468] [D loss: 0.6803686618804932] [G loss: 0.7221386432647705]\n",
      "[Epoch 35/100] [Batch 300/468] [D loss: 0.6485068798065186] [G loss: 0.8603338599205017]\n",
      "[Epoch 35/100] [Batch 400/468] [D loss: 0.712677001953125] [G loss: 0.8975911140441895]\n",
      "[Epoch 36/100] [Batch 0/468] [D loss: 0.6639866232872009] [G loss: 0.7686037421226501]\n",
      "[Epoch 36/100] [Batch 100/468] [D loss: 0.6384089589118958] [G loss: 0.9213650226593018]\n",
      "[Epoch 36/100] [Batch 200/468] [D loss: 0.6221980452537537] [G loss: 0.7832472324371338]\n",
      "[Epoch 36/100] [Batch 300/468] [D loss: 0.6622529029846191] [G loss: 0.9292915463447571]\n",
      "[Epoch 36/100] [Batch 400/468] [D loss: 0.6801581382751465] [G loss: 0.7236874103546143]\n",
      "[Epoch 37/100] [Batch 0/468] [D loss: 0.6401777863502502] [G loss: 0.7448943853378296]\n",
      "[Epoch 37/100] [Batch 100/468] [D loss: 0.6407201290130615] [G loss: 0.7625585794448853]\n",
      "[Epoch 37/100] [Batch 200/468] [D loss: 0.6741465330123901] [G loss: 0.8102666139602661]\n",
      "[Epoch 37/100] [Batch 300/468] [D loss: 0.6716399192810059] [G loss: 0.6977737545967102]\n",
      "[Epoch 37/100] [Batch 400/468] [D loss: 0.640958845615387] [G loss: 0.7214488983154297]\n",
      "[Epoch 38/100] [Batch 0/468] [D loss: 0.6547535061836243] [G loss: 0.8474404811859131]\n",
      "[Epoch 38/100] [Batch 100/468] [D loss: 0.6606477499008179] [G loss: 0.933541476726532]\n",
      "[Epoch 38/100] [Batch 200/468] [D loss: 0.6400281190872192] [G loss: 0.8062670230865479]\n",
      "[Epoch 38/100] [Batch 300/468] [D loss: 0.672562837600708] [G loss: 0.9264394044876099]\n",
      "[Epoch 38/100] [Batch 400/468] [D loss: 0.6586829423904419] [G loss: 0.8753655552864075]\n",
      "[Epoch 39/100] [Batch 0/468] [D loss: 0.6723659634590149] [G loss: 0.8115628361701965]\n",
      "[Epoch 39/100] [Batch 100/468] [D loss: 0.6806805729866028] [G loss: 0.7188236713409424]\n",
      "[Epoch 39/100] [Batch 200/468] [D loss: 0.6716185212135315] [G loss: 0.8421496152877808]\n",
      "[Epoch 39/100] [Batch 300/468] [D loss: 0.6677573323249817] [G loss: 0.6741272211074829]\n",
      "[Epoch 39/100] [Batch 400/468] [D loss: 0.6734647750854492] [G loss: 0.7340514063835144]\n",
      "[Epoch 40/100] [Batch 0/468] [D loss: 0.6744579672813416] [G loss: 0.8312948942184448]\n",
      "[Epoch 40/100] [Batch 100/468] [D loss: 0.6790604591369629] [G loss: 0.8052400946617126]\n",
      "[Epoch 40/100] [Batch 200/468] [D loss: 0.6752870082855225] [G loss: 0.7679860591888428]\n",
      "[Epoch 40/100] [Batch 300/468] [D loss: 0.6686825752258301] [G loss: 0.7985718250274658]\n",
      "[Epoch 40/100] [Batch 400/468] [D loss: 0.6393033266067505] [G loss: 0.7168115377426147]\n",
      "[Epoch 41/100] [Batch 0/468] [D loss: 0.6337981224060059] [G loss: 0.8397882580757141]\n",
      "[Epoch 41/100] [Batch 100/468] [D loss: 0.6445274353027344] [G loss: 0.8452396988868713]\n",
      "[Epoch 41/100] [Batch 200/468] [D loss: 0.6911677718162537] [G loss: 0.8807636499404907]\n",
      "[Epoch 41/100] [Batch 300/468] [D loss: 0.6568586826324463] [G loss: 0.7403262853622437]\n",
      "[Epoch 41/100] [Batch 400/468] [D loss: 0.656632661819458] [G loss: 0.7178479433059692]\n",
      "[Epoch 42/100] [Batch 0/468] [D loss: 0.6514965295791626] [G loss: 0.7618430852890015]\n",
      "[Epoch 42/100] [Batch 100/468] [D loss: 0.6813388466835022] [G loss: 0.7496733665466309]\n",
      "[Epoch 42/100] [Batch 200/468] [D loss: 0.6309487223625183] [G loss: 0.7778915762901306]\n",
      "[Epoch 42/100] [Batch 300/468] [D loss: 0.6497061252593994] [G loss: 0.7491800785064697]\n",
      "[Epoch 42/100] [Batch 400/468] [D loss: 0.6486338376998901] [G loss: 0.7975190877914429]\n",
      "[Epoch 43/100] [Batch 0/468] [D loss: 0.6599512696266174] [G loss: 0.8587251901626587]\n",
      "[Epoch 43/100] [Batch 100/468] [D loss: 0.6642478704452515] [G loss: 0.9429000616073608]\n",
      "[Epoch 43/100] [Batch 200/468] [D loss: 0.6503293514251709] [G loss: 0.6991347670555115]\n",
      "[Epoch 43/100] [Batch 300/468] [D loss: 0.6471325755119324] [G loss: 0.7984106540679932]\n",
      "[Epoch 43/100] [Batch 400/468] [D loss: 0.6430453062057495] [G loss: 0.7811181545257568]\n",
      "[Epoch 44/100] [Batch 0/468] [D loss: 0.6506580114364624] [G loss: 0.8617630004882812]\n",
      "[Epoch 44/100] [Batch 100/468] [D loss: 0.6203315258026123] [G loss: 0.7791845798492432]\n",
      "[Epoch 44/100] [Batch 200/468] [D loss: 0.6089453101158142] [G loss: 0.7707862854003906]\n",
      "[Epoch 44/100] [Batch 300/468] [D loss: 0.6457716226577759] [G loss: 0.7795084714889526]\n",
      "[Epoch 44/100] [Batch 400/468] [D loss: 0.6584139466285706] [G loss: 0.7945508360862732]\n",
      "[Epoch 45/100] [Batch 0/468] [D loss: 0.6321761608123779] [G loss: 0.9236219525337219]\n",
      "[Epoch 45/100] [Batch 100/468] [D loss: 0.6572202444076538] [G loss: 0.7734665870666504]\n",
      "[Epoch 45/100] [Batch 200/468] [D loss: 0.6541092395782471] [G loss: 0.8160148859024048]\n",
      "[Epoch 45/100] [Batch 300/468] [D loss: 0.6485819816589355] [G loss: 0.7713148593902588]\n",
      "[Epoch 45/100] [Batch 400/468] [D loss: 0.687962532043457] [G loss: 0.8113570213317871]\n",
      "[Epoch 46/100] [Batch 0/468] [D loss: 0.6416301727294922] [G loss: 0.8884953856468201]\n",
      "[Epoch 46/100] [Batch 100/468] [D loss: 0.6309014558792114] [G loss: 0.8956482410430908]\n",
      "[Epoch 46/100] [Batch 200/468] [D loss: 0.682453453540802] [G loss: 0.83653724193573]\n",
      "[Epoch 46/100] [Batch 300/468] [D loss: 0.673777163028717] [G loss: 0.8454052209854126]\n",
      "[Epoch 46/100] [Batch 400/468] [D loss: 0.6790860891342163] [G loss: 0.8716288208961487]\n",
      "[Epoch 47/100] [Batch 0/468] [D loss: 0.6438164114952087] [G loss: 0.796736478805542]\n",
      "[Epoch 47/100] [Batch 100/468] [D loss: 0.6532849073410034] [G loss: 0.9434832334518433]\n",
      "[Epoch 47/100] [Batch 200/468] [D loss: 0.6629173755645752] [G loss: 0.6935609579086304]\n",
      "[Epoch 47/100] [Batch 300/468] [D loss: 0.6779763102531433] [G loss: 0.7945722341537476]\n",
      "[Epoch 47/100] [Batch 400/468] [D loss: 0.641055166721344] [G loss: 0.8295592665672302]\n",
      "[Epoch 48/100] [Batch 0/468] [D loss: 0.6215313673019409] [G loss: 0.8270232081413269]\n",
      "[Epoch 48/100] [Batch 100/468] [D loss: 0.6443023681640625] [G loss: 0.9086716175079346]\n",
      "[Epoch 48/100] [Batch 200/468] [D loss: 0.6510854363441467] [G loss: 0.7967650294303894]\n",
      "[Epoch 48/100] [Batch 300/468] [D loss: 0.6498677730560303] [G loss: 0.8855371475219727]\n",
      "[Epoch 48/100] [Batch 400/468] [D loss: 0.6341005563735962] [G loss: 0.8554298877716064]\n",
      "[Epoch 49/100] [Batch 0/468] [D loss: 0.6423084735870361] [G loss: 0.8419526815414429]\n",
      "[Epoch 49/100] [Batch 100/468] [D loss: 0.6416733264923096] [G loss: 0.855523943901062]\n",
      "[Epoch 49/100] [Batch 200/468] [D loss: 0.6507880687713623] [G loss: 0.7735059261322021]\n",
      "[Epoch 49/100] [Batch 300/468] [D loss: 0.6429692506790161] [G loss: 0.8204336166381836]\n",
      "[Epoch 49/100] [Batch 400/468] [D loss: 0.6471337080001831] [G loss: 0.7660555839538574]\n",
      "[Epoch 50/100] [Batch 0/468] [D loss: 0.6423815488815308] [G loss: 0.838996946811676]\n",
      "[Epoch 50/100] [Batch 100/468] [D loss: 0.6212093830108643] [G loss: 0.8170318603515625]\n",
      "[Epoch 50/100] [Batch 200/468] [D loss: 0.6341840028762817] [G loss: 0.7363924980163574]\n",
      "[Epoch 50/100] [Batch 300/468] [D loss: 0.6418068408966064] [G loss: 0.9515785574913025]\n",
      "[Epoch 50/100] [Batch 400/468] [D loss: 0.6494905948638916] [G loss: 0.8594257831573486]\n",
      "[Epoch 51/100] [Batch 0/468] [D loss: 0.6478633880615234] [G loss: 0.9065660238265991]\n",
      "[Epoch 51/100] [Batch 100/468] [D loss: 0.6530651450157166] [G loss: 0.8121498823165894]\n",
      "[Epoch 51/100] [Batch 200/468] [D loss: 0.6563597917556763] [G loss: 0.7581194639205933]\n",
      "[Epoch 51/100] [Batch 300/468] [D loss: 0.6153700351715088] [G loss: 0.929767370223999]\n",
      "[Epoch 51/100] [Batch 400/468] [D loss: 0.6373106837272644] [G loss: 0.8524603843688965]\n",
      "[Epoch 52/100] [Batch 0/468] [D loss: 0.6348779201507568] [G loss: 0.859467089176178]\n",
      "[Epoch 52/100] [Batch 100/468] [D loss: 0.6688998937606812] [G loss: 0.820366621017456]\n",
      "[Epoch 52/100] [Batch 200/468] [D loss: 0.6257348656654358] [G loss: 0.7983278036117554]\n",
      "[Epoch 52/100] [Batch 300/468] [D loss: 0.6734483242034912] [G loss: 0.8073810338973999]\n",
      "[Epoch 52/100] [Batch 400/468] [D loss: 0.6665952205657959] [G loss: 0.7336159348487854]\n",
      "[Epoch 53/100] [Batch 0/468] [D loss: 0.6254518628120422] [G loss: 0.8165058493614197]\n",
      "[Epoch 53/100] [Batch 100/468] [D loss: 0.6095805168151855] [G loss: 0.7985305786132812]\n",
      "[Epoch 53/100] [Batch 200/468] [D loss: 0.6421452760696411] [G loss: 0.769652247428894]\n",
      "[Epoch 53/100] [Batch 300/468] [D loss: 0.643459677696228] [G loss: 0.9008573293685913]\n",
      "[Epoch 53/100] [Batch 400/468] [D loss: 0.6181958317756653] [G loss: 0.8682857751846313]\n",
      "[Epoch 54/100] [Batch 0/468] [D loss: 0.630524754524231] [G loss: 1.0299983024597168]\n",
      "[Epoch 54/100] [Batch 100/468] [D loss: 0.6490026712417603] [G loss: 0.7873014211654663]\n",
      "[Epoch 54/100] [Batch 200/468] [D loss: 0.6138245463371277] [G loss: 0.8025001287460327]\n",
      "[Epoch 54/100] [Batch 300/468] [D loss: 0.6590067148208618] [G loss: 0.9117387533187866]\n",
      "[Epoch 54/100] [Batch 400/468] [D loss: 0.6411293745040894] [G loss: 0.8448777794837952]\n",
      "[Epoch 55/100] [Batch 0/468] [D loss: 0.6560437679290771] [G loss: 0.7879663705825806]\n",
      "[Epoch 55/100] [Batch 100/468] [D loss: 0.6404008865356445] [G loss: 0.8196858763694763]\n",
      "[Epoch 55/100] [Batch 200/468] [D loss: 0.6569159030914307] [G loss: 0.7943738698959351]\n",
      "[Epoch 55/100] [Batch 300/468] [D loss: 0.6627634763717651] [G loss: 0.894326388835907]\n",
      "[Epoch 55/100] [Batch 400/468] [D loss: 0.6554473042488098] [G loss: 0.7910155057907104]\n",
      "[Epoch 56/100] [Batch 0/468] [D loss: 0.638205349445343] [G loss: 0.8669134378433228]\n",
      "[Epoch 56/100] [Batch 100/468] [D loss: 0.6687637567520142] [G loss: 0.796552300453186]\n",
      "[Epoch 56/100] [Batch 200/468] [D loss: 0.635122537612915] [G loss: 0.9223414659500122]\n",
      "[Epoch 56/100] [Batch 300/468] [D loss: 0.6129844188690186] [G loss: 0.8206295967102051]\n",
      "[Epoch 56/100] [Batch 400/468] [D loss: 0.6222206950187683] [G loss: 0.9636012315750122]\n",
      "[Epoch 57/100] [Batch 0/468] [D loss: 0.6594946980476379] [G loss: 0.7776864171028137]\n",
      "[Epoch 57/100] [Batch 100/468] [D loss: 0.654650092124939] [G loss: 0.7888153791427612]\n",
      "[Epoch 57/100] [Batch 200/468] [D loss: 0.6290708184242249] [G loss: 0.7680892944335938]\n",
      "[Epoch 57/100] [Batch 300/468] [D loss: 0.6553629040718079] [G loss: 0.7724690437316895]\n",
      "[Epoch 57/100] [Batch 400/468] [D loss: 0.6647829413414001] [G loss: 0.9256641268730164]\n",
      "[Epoch 58/100] [Batch 0/468] [D loss: 0.6670602560043335] [G loss: 0.8933825492858887]\n",
      "[Epoch 58/100] [Batch 100/468] [D loss: 0.6543726325035095] [G loss: 0.8684539198875427]\n",
      "[Epoch 58/100] [Batch 200/468] [D loss: 0.625062108039856] [G loss: 0.8162953853607178]\n",
      "[Epoch 58/100] [Batch 300/468] [D loss: 0.6481709480285645] [G loss: 0.8389137983322144]\n",
      "[Epoch 58/100] [Batch 400/468] [D loss: 0.6282187700271606] [G loss: 0.8395304679870605]\n",
      "[Epoch 59/100] [Batch 0/468] [D loss: 0.6186741590499878] [G loss: 0.7539542317390442]\n",
      "[Epoch 59/100] [Batch 100/468] [D loss: 0.648158073425293] [G loss: 0.8988916277885437]\n",
      "[Epoch 59/100] [Batch 200/468] [D loss: 0.656603217124939] [G loss: 0.885452389717102]\n",
      "[Epoch 59/100] [Batch 300/468] [D loss: 0.6346300840377808] [G loss: 0.9184544086456299]\n",
      "[Epoch 59/100] [Batch 400/468] [D loss: 0.6586831212043762] [G loss: 0.8105798959732056]\n",
      "[Epoch 60/100] [Batch 0/468] [D loss: 0.6504093408584595] [G loss: 0.6813569664955139]\n",
      "[Epoch 60/100] [Batch 100/468] [D loss: 0.6899104714393616] [G loss: 0.8216797113418579]\n",
      "[Epoch 60/100] [Batch 200/468] [D loss: 0.624205470085144] [G loss: 0.8397040963172913]\n",
      "[Epoch 60/100] [Batch 300/468] [D loss: 0.6441501975059509] [G loss: 0.8172106742858887]\n",
      "[Epoch 60/100] [Batch 400/468] [D loss: 0.6590380668640137] [G loss: 0.7907474040985107]\n",
      "[Epoch 61/100] [Batch 0/468] [D loss: 0.6171693205833435] [G loss: 0.7531415224075317]\n",
      "[Epoch 61/100] [Batch 100/468] [D loss: 0.6471157670021057] [G loss: 0.7903767824172974]\n",
      "[Epoch 61/100] [Batch 200/468] [D loss: 0.6392822861671448] [G loss: 0.8631194829940796]\n",
      "[Epoch 61/100] [Batch 300/468] [D loss: 0.6850221753120422] [G loss: 0.7983023524284363]\n",
      "[Epoch 61/100] [Batch 400/468] [D loss: 0.6374600529670715] [G loss: 0.8392496109008789]\n",
      "[Epoch 62/100] [Batch 0/468] [D loss: 0.622793972492218] [G loss: 0.8393549919128418]\n",
      "[Epoch 62/100] [Batch 100/468] [D loss: 0.5848788022994995] [G loss: 1.044620394706726]\n",
      "[Epoch 62/100] [Batch 200/468] [D loss: 0.6507730484008789] [G loss: 0.83856201171875]\n",
      "[Epoch 62/100] [Batch 300/468] [D loss: 0.6323898434638977] [G loss: 0.8082083463668823]\n",
      "[Epoch 62/100] [Batch 400/468] [D loss: 0.6178874969482422] [G loss: 0.8734365701675415]\n",
      "[Epoch 63/100] [Batch 0/468] [D loss: 0.6102854609489441] [G loss: 0.8468579649925232]\n",
      "[Epoch 63/100] [Batch 100/468] [D loss: 0.684190571308136] [G loss: 0.8496770858764648]\n",
      "[Epoch 63/100] [Batch 200/468] [D loss: 0.6331148147583008] [G loss: 0.8263847827911377]\n",
      "[Epoch 63/100] [Batch 300/468] [D loss: 0.641743004322052] [G loss: 0.8977911472320557]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[50], line 18\u001B[0m\n\u001B[1;32m     16\u001B[0m z \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(batch_size, latent_dim, device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m     17\u001B[0m gen_labels \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m0\u001B[39m, num_classes, (batch_size,), device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[0;32m---> 18\u001B[0m gen_imgs \u001B[38;5;241m=\u001B[39m \u001B[43mgenerator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgen_labels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m validity \u001B[38;5;241m=\u001B[39m discriminator(gen_imgs, gen_labels)\n\u001B[1;32m     20\u001B[0m g_loss \u001B[38;5;241m=\u001B[39m adversarial_loss(validity, valid)\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[48], line 29\u001B[0m, in \u001B[0;36mGenerator.forward\u001B[0;34m(self, noise, labels)\u001B[0m\n\u001B[1;32m     27\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39ml1(gen_input)\n\u001B[1;32m     28\u001B[0m out \u001B[38;5;241m=\u001B[39m out\u001B[38;5;241m.\u001B[39mview(out\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m128\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minit_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minit_size)\n\u001B[0;32m---> 29\u001B[0m img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.11/site-packages/torch/nn/modules/activation.py:356\u001B[0m, in \u001B[0;36mTanh.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    355\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 356\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtanh\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "generator = Generator(img_channels=1).to(device)\n",
    "discriminator = Discriminator(img_channels=1).to(device)\n",
    "\n",
    "adversarial_loss = nn.BCELoss()\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (imgs, labels) in enumerate(train_loader):\n",
    "        batch_size = imgs.size(0)\n",
    "        real_imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        valid = torch.ones(batch_size, 1, device=device, dtype=torch.float32)\n",
    "        fake = torch.zeros(batch_size, 1, device=device, dtype=torch.float32)\n",
    "        optimizer_G.zero_grad()\n",
    "        z = torch.randn(batch_size, latent_dim, device=device)\n",
    "        gen_labels = torch.randint(0, num_classes, (batch_size,), device=device)\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "        validity = discriminator(gen_imgs, gen_labels)\n",
    "        g_loss = adversarial_loss(validity, valid)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        optimizer_D.zero_grad()\n",
    "        validity_real = discriminator(real_imgs, labels)\n",
    "        d_real_loss = adversarial_loss(validity_real, valid)\n",
    "        validity_fake = discriminator(gen_imgs.detach(), gen_labels)\n",
    "        d_fake_loss = adversarial_loss(validity_fake, fake)\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        if i % 100 == 0:\n",
    "            print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(train_loader)}] \"\n",
    "                  f\"[D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
    "    generate_images(generator, epoch, n_images, latent_dim, digit=2)\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
