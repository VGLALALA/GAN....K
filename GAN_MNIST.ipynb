{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T01:39:54.424168029Z",
     "start_time": "2024-05-16T01:39:54.406294850Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "lr = 0.0002\n",
    "batch_size = 128\n",
    "image_size = 28*28\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T01:39:54.456156196Z",
     "start_time": "2024-05-16T01:39:54.455892080Z"
    }
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T01:39:54.456296730Z",
     "start_time": "2024-05-16T01:39:54.456104028Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Generator\n",
    "# class Generator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(latent_dim, 256),\n",
    "#             nn.BatchNorm1d(256),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Linear(256, 512),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Linear(512, 1024),\n",
    "#             nn.BatchNorm1d(1024),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Linear(1024, image_size),\n",
    "#             nn.BatchNorm1d(image_size),\n",
    "#             nn.Tanh()\n",
    "#         )\n",
    "# \n",
    "#     def forward(self, z):\n",
    "#         return self.model(z)\n",
    "# \n",
    "# # Discriminator\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(image_size, 1024),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(1024, 512),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(512, 256),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(256, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "# \n",
    "#     def forward(self, img):\n",
    "#         img_flat = img.view(img.size(0), -1)\n",
    "#         return self.model(img_flat)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_channels=1):\n",
    "        super(Generator, self).__init__()\n",
    "        self.init_size = 7  # Initial size before upsampling\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, img_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.model(out)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer to output a single validity score per image\n",
    "        self.adv_layer = nn.Sequential(\n",
    "            nn.Linear(512 * 2 * 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.size(0), -1)  # Flatten the output\n",
    "        validity = self.adv_layer(out)\n",
    "        return validity\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T01:47:16.518982802Z",
     "start_time": "2024-05-16T01:39:54.456250674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [400/468], D Loss: 0.9690850973129272, G Loss: 1.4154253005981445\n",
      "Epoch [2/100], Step [400/468], D Loss: 0.6552090644836426, G Loss: 1.7175602912902832\n",
      "Epoch [3/100], Step [400/468], D Loss: 0.9909242391586304, G Loss: 1.3598992824554443\n",
      "Epoch [4/100], Step [400/468], D Loss: 1.1192089319229126, G Loss: 1.3783931732177734\n",
      "Epoch [5/100], Step [400/468], D Loss: 0.8978086709976196, G Loss: 1.32565438747406\n",
      "Epoch [6/100], Step [400/468], D Loss: 1.1752640008926392, G Loss: 1.1328778266906738\n",
      "Epoch [7/100], Step [400/468], D Loss: 1.0285788774490356, G Loss: 1.328322410583496\n",
      "Epoch [8/100], Step [400/468], D Loss: 0.9722626805305481, G Loss: 1.393446683883667\n",
      "Epoch [9/100], Step [400/468], D Loss: 1.0495445728302002, G Loss: 1.2878639698028564\n",
      "Epoch [10/100], Step [400/468], D Loss: 0.9416558742523193, G Loss: 1.592057466506958\n",
      "Epoch [11/100], Step [400/468], D Loss: 0.7443684339523315, G Loss: 1.6719391345977783\n",
      "Epoch [12/100], Step [400/468], D Loss: 0.9260530471801758, G Loss: 1.4548306465148926\n",
      "Epoch [13/100], Step [400/468], D Loss: 0.9272506833076477, G Loss: 1.3352391719818115\n",
      "Epoch [14/100], Step [400/468], D Loss: 1.0713582038879395, G Loss: 1.5746188163757324\n",
      "Epoch [15/100], Step [400/468], D Loss: 0.8872422575950623, G Loss: 1.3763678073883057\n",
      "Epoch [16/100], Step [400/468], D Loss: 0.885824978351593, G Loss: 1.503164291381836\n",
      "Epoch [17/100], Step [400/468], D Loss: 0.9820176959037781, G Loss: 1.3379698991775513\n",
      "Epoch [18/100], Step [400/468], D Loss: 0.9678897857666016, G Loss: 1.6115119457244873\n",
      "Epoch [19/100], Step [400/468], D Loss: 1.1398804187774658, G Loss: 1.3960349559783936\n",
      "Epoch [20/100], Step [400/468], D Loss: 1.1741622686386108, G Loss: 1.2560608386993408\n",
      "Epoch [21/100], Step [400/468], D Loss: 0.855643630027771, G Loss: 1.6713337898254395\n",
      "Epoch [22/100], Step [400/468], D Loss: 1.0194365978240967, G Loss: 1.4414894580841064\n",
      "Epoch [23/100], Step [400/468], D Loss: 1.0001565217971802, G Loss: 1.11091947555542\n",
      "Epoch [24/100], Step [400/468], D Loss: 1.0456066131591797, G Loss: 1.2948782444000244\n",
      "Epoch [25/100], Step [400/468], D Loss: 0.969923734664917, G Loss: 1.3575284481048584\n",
      "Epoch [26/100], Step [400/468], D Loss: 1.1201732158660889, G Loss: 1.3725296258926392\n",
      "Epoch [27/100], Step [400/468], D Loss: 0.9364780187606812, G Loss: 1.3585376739501953\n",
      "Epoch [28/100], Step [400/468], D Loss: 1.0048679113388062, G Loss: 1.3982775211334229\n",
      "Epoch [29/100], Step [400/468], D Loss: 1.0361284017562866, G Loss: 1.4495753049850464\n",
      "Epoch [30/100], Step [400/468], D Loss: 1.0461363792419434, G Loss: 1.325268030166626\n",
      "Epoch [31/100], Step [400/468], D Loss: 0.9821859002113342, G Loss: 1.3104040622711182\n",
      "Epoch [32/100], Step [400/468], D Loss: 1.0821735858917236, G Loss: 1.5079123973846436\n",
      "Epoch [33/100], Step [400/468], D Loss: 0.9551135301589966, G Loss: 1.5015754699707031\n",
      "Epoch [34/100], Step [400/468], D Loss: 1.1485602855682373, G Loss: 1.5442036390304565\n",
      "Epoch [35/100], Step [400/468], D Loss: 0.9960087537765503, G Loss: 1.356656551361084\n",
      "Epoch [36/100], Step [400/468], D Loss: 1.0962817668914795, G Loss: 1.2897043228149414\n",
      "Epoch [37/100], Step [400/468], D Loss: 0.9592118263244629, G Loss: 1.3622007369995117\n",
      "Epoch [38/100], Step [400/468], D Loss: 1.0132477283477783, G Loss: 1.2924389839172363\n",
      "Epoch [39/100], Step [400/468], D Loss: 1.0818166732788086, G Loss: 1.179084062576294\n",
      "Epoch [40/100], Step [400/468], D Loss: 1.079901933670044, G Loss: 1.2822242975234985\n",
      "Epoch [41/100], Step [400/468], D Loss: 0.9365997314453125, G Loss: 1.5450494289398193\n",
      "Epoch [42/100], Step [400/468], D Loss: 1.0040667057037354, G Loss: 1.2961275577545166\n",
      "Epoch [43/100], Step [400/468], D Loss: 1.071622610092163, G Loss: 1.237184762954712\n",
      "Epoch [44/100], Step [400/468], D Loss: 1.0414984226226807, G Loss: 1.2190449237823486\n",
      "Epoch [45/100], Step [400/468], D Loss: 1.0895737409591675, G Loss: 1.0890421867370605\n",
      "Epoch [46/100], Step [400/468], D Loss: 1.037132740020752, G Loss: 1.211549997329712\n",
      "Epoch [47/100], Step [400/468], D Loss: 1.1577050685882568, G Loss: 1.3200359344482422\n",
      "Epoch [48/100], Step [400/468], D Loss: 1.048580288887024, G Loss: 1.2988206148147583\n",
      "Epoch [49/100], Step [400/468], D Loss: 1.0519156455993652, G Loss: 1.1548880338668823\n",
      "Epoch [50/100], Step [400/468], D Loss: 0.8627015948295593, G Loss: 1.568903923034668\n",
      "Epoch [51/100], Step [400/468], D Loss: 1.070123553276062, G Loss: 1.3316662311553955\n",
      "Epoch [52/100], Step [400/468], D Loss: 0.9537926912307739, G Loss: 1.4039084911346436\n",
      "Epoch [53/100], Step [400/468], D Loss: 1.0519118309020996, G Loss: 1.4372025728225708\n",
      "Epoch [54/100], Step [400/468], D Loss: 0.9868028163909912, G Loss: 1.3262252807617188\n",
      "Epoch [55/100], Step [400/468], D Loss: 1.0759663581848145, G Loss: 1.2664694786071777\n",
      "Epoch [56/100], Step [400/468], D Loss: 1.1261746883392334, G Loss: 1.3157854080200195\n",
      "Epoch [57/100], Step [400/468], D Loss: 1.046393871307373, G Loss: 1.234411358833313\n",
      "Epoch [58/100], Step [400/468], D Loss: 0.9817912578582764, G Loss: 1.3127835988998413\n",
      "Epoch [59/100], Step [400/468], D Loss: 1.0106942653656006, G Loss: 1.3557679653167725\n",
      "Epoch [60/100], Step [400/468], D Loss: 1.1199839115142822, G Loss: 1.1976723670959473\n",
      "Epoch [61/100], Step [400/468], D Loss: 1.078113079071045, G Loss: 1.251220941543579\n",
      "Epoch [62/100], Step [400/468], D Loss: 0.9594345688819885, G Loss: 1.2629724740982056\n",
      "Epoch [63/100], Step [400/468], D Loss: 0.9317812919616699, G Loss: 1.3043382167816162\n",
      "Epoch [64/100], Step [400/468], D Loss: 0.9650473594665527, G Loss: 1.2863833904266357\n",
      "Epoch [65/100], Step [400/468], D Loss: 0.9358627796173096, G Loss: 1.517958164215088\n",
      "Epoch [66/100], Step [400/468], D Loss: 1.1584302186965942, G Loss: 1.2500457763671875\n",
      "Epoch [67/100], Step [400/468], D Loss: 1.0521714687347412, G Loss: 1.2824102640151978\n",
      "Epoch [68/100], Step [400/468], D Loss: 0.9766710996627808, G Loss: 1.3194105625152588\n",
      "Epoch [69/100], Step [400/468], D Loss: 1.053950548171997, G Loss: 1.1778086423873901\n",
      "Epoch [70/100], Step [400/468], D Loss: 1.0460810661315918, G Loss: 1.201257586479187\n",
      "Epoch [71/100], Step [400/468], D Loss: 0.890941858291626, G Loss: 1.2499464750289917\n",
      "Epoch [72/100], Step [400/468], D Loss: 1.0372443199157715, G Loss: 1.2673699855804443\n",
      "Epoch [73/100], Step [400/468], D Loss: 0.9638955593109131, G Loss: 1.140202283859253\n",
      "Epoch [74/100], Step [400/468], D Loss: 1.0399543046951294, G Loss: 1.1708875894546509\n",
      "Epoch [75/100], Step [400/468], D Loss: 0.9530896544456482, G Loss: 1.2184336185455322\n",
      "Epoch [76/100], Step [400/468], D Loss: 1.1086500883102417, G Loss: 1.267500400543213\n",
      "Epoch [77/100], Step [400/468], D Loss: 0.9986802339553833, G Loss: 1.2228713035583496\n",
      "Epoch [78/100], Step [400/468], D Loss: 1.1238526105880737, G Loss: 1.3838927745819092\n",
      "Epoch [79/100], Step [400/468], D Loss: 0.9792543053627014, G Loss: 1.341910481452942\n",
      "Epoch [80/100], Step [400/468], D Loss: 1.138587236404419, G Loss: 1.1657054424285889\n",
      "Epoch [81/100], Step [400/468], D Loss: 0.975866973400116, G Loss: 1.4729950428009033\n",
      "Epoch [82/100], Step [400/468], D Loss: 1.0469685792922974, G Loss: 1.216972827911377\n",
      "Epoch [83/100], Step [400/468], D Loss: 1.0734302997589111, G Loss: 1.2346301078796387\n",
      "Epoch [84/100], Step [400/468], D Loss: 1.0542373657226562, G Loss: 1.1535018682479858\n",
      "Epoch [85/100], Step [400/468], D Loss: 0.9845708012580872, G Loss: 1.4253125190734863\n",
      "Epoch [86/100], Step [400/468], D Loss: 1.0271399021148682, G Loss: 1.2946836948394775\n",
      "Epoch [87/100], Step [400/468], D Loss: 1.020664930343628, G Loss: 1.1410986185073853\n",
      "Epoch [88/100], Step [400/468], D Loss: 0.9360784292221069, G Loss: 1.286855697631836\n",
      "Epoch [89/100], Step [400/468], D Loss: 1.0073864459991455, G Loss: 1.0900260210037231\n",
      "Epoch [90/100], Step [400/468], D Loss: 0.9494088888168335, G Loss: 1.2371189594268799\n",
      "Epoch [91/100], Step [400/468], D Loss: 1.1959679126739502, G Loss: 1.1485557556152344\n",
      "Epoch [92/100], Step [400/468], D Loss: 1.0832926034927368, G Loss: 1.1888482570648193\n",
      "Epoch [93/100], Step [400/468], D Loss: 1.1603138446807861, G Loss: 1.2422337532043457\n",
      "Epoch [94/100], Step [400/468], D Loss: 1.1857995986938477, G Loss: 1.2689582109451294\n",
      "Epoch [95/100], Step [400/468], D Loss: 1.0337300300598145, G Loss: 1.2803258895874023\n",
      "Epoch [96/100], Step [400/468], D Loss: 0.9870278835296631, G Loss: 1.2666842937469482\n",
      "Epoch [97/100], Step [400/468], D Loss: 1.0988948345184326, G Loss: 1.2337164878845215\n",
      "Epoch [98/100], Step [400/468], D Loss: 1.0802851915359497, G Loss: 1.0938844680786133\n",
      "Epoch [99/100], Step [400/468], D Loss: 1.1862599849700928, G Loss: 1.2989811897277832\n",
      "Epoch [100/100], Step [400/468], D Loss: 1.0198819637298584, G Loss: 1.3530638217926025\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Optimizers\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        real_images = images.to(device)\n",
    "        batch_size = real_images.size(0)\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        # --------- Train the Discriminator --------- #\n",
    "        d_optimizer.zero_grad()\n",
    "        outputs = discriminator(real_images)\n",
    "        d_real_loss = criterion(outputs, real_labels)\n",
    "        z = torch.randn(batch_size, latent_dim).to(device)\n",
    "        fake_images = generator(z)\n",
    "        outputs = discriminator(fake_images.detach())\n",
    "        d_fake_loss = criterion(outputs, fake_labels)\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # --------- Train the Generator --------- #\n",
    "        g_optimizer.zero_grad()\n",
    "        outputs = discriminator(fake_images)\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        if (i+1) % 400 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], D Loss: {d_loss.item()}, G Loss: {g_loss.item()}')\n",
    "\n",
    "    # Save generated images every epoch\n",
    "    save_image(fake_images.data[:25], f'./data/gan/fake_image_{epoch+1:03d}.png', nrow=5, normalize=True)\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
